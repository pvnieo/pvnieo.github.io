<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog | pvnieo</title>
    <link>https://pvnieo.github.io/post/</link>
      <atom:link href="https://pvnieo.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Blog</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Souhaib Attaiki, made with lots of ❤️ and steaming hot ☕️.</copyright><lastBuildDate>Wed, 01 Jan 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://pvnieo.github.io/img/icon.png</url>
      <title>Blog</title>
      <link>https://pvnieo.github.io/post/</link>
    </image>
    
    <item>
      <title>Learn Tic Tac Toe</title>
      <link>https://pvnieo.github.io/post/learn-tic-tac-toe/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://pvnieo.github.io/post/learn-tic-tac-toe/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;As my 7 year old nephew discovered the famous strategy game &amp;quot;Tic Tac Toe&amp;quot;, I wanted to help her by creating an AI that she can play against, which has several levels of difficulty so that she can learn the game step by step.&lt;/p&gt;

&lt;p&gt;Tic Tac Toe, for those who don&#39;t know, is a paper and pencil game for two players, X and O, who take turns marking the spaces in a 3×3 grid. The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row is the winner (see image bellow).
&lt;figure&gt;&lt;img src=&#34;./tic.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h1 id=&#34;game-interface&#34;&gt;Game interface&lt;/h1&gt;

&lt;p&gt;As my goal is to help her master the game, I didn&#39;t dwell on the game interface. I implemented the latter in python, using the &lt;code&gt;pygame&lt;/code&gt; library.&lt;/p&gt;

&lt;p&gt;The interface is a 3×3 grid, where the player can put an &amp;quot;X&amp;quot; or &amp;quot;O&amp;quot; in a box by clicking on it, or it will be put automatically if it is the turn of an AI. At the end of the game, a line indicating the winning combination (if it exists) will be drawn in red (see picture below). Before the game starts, the player can choose the type and level of the AI he wants to play against, as well as the number of matches he wants to play. You can find my implementation &lt;a href=&#34;https://github.com/pvnieo/Learn-Noughts-and-Crosses&#34;&gt;here&lt;/a&gt;.
&lt;figure&gt;&lt;img src=&#34;./image.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h1 id=&#34;tictactoe-with-the-minimax-algorithm&#34;&gt;Tic-Tac-Toe with the Minimax Algorithm&lt;/h1&gt;

&lt;p&gt;As Tic-Tac-Toe is a &lt;strong&gt;zero-sum&lt;/strong&gt; (each participant’s gain is equal to the other participants’ losses), &lt;strong&gt;perfect information&lt;/strong&gt; game (everything is known about the current game state), and has a small number of different states (5,478 legal states&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;), the minmax algorithm would be a perfect and fast algorithm for this case.&lt;/p&gt;

&lt;h4 id=&#34;minmax-algorithm&#34;&gt;Minmax algorithm&lt;/h4&gt;

&lt;p&gt;The basic idea behind minimax is that we want to know what is the best move to play when we assume our opponent will play the best moves possible. In Minimax the two players are called maximizer and minimizer. The maximizer tries to get the highest score possible while the minimizer tries to do the opposite and get the lowest score possible.&lt;/p&gt;

&lt;p&gt;Every board state has a value associated with it. In a given state if the maximizer has upper hand then, the score of the board will tend to be some positive value. If the minimizer has the upper hand in that board state then it will tend to be some negative value. In the case of Tic Tac Toe, there are only three board scores: either the maximizer wins (+1), or the maximizer loses (-1), or there is a tie, or the game is not yet over (0). The minmax algorithm explores the game from any starting position until it reaches all possible endgame states,, creating a tree, and then backtrack the best move to make, assuming that the opponent will always play his best move.&lt;/p&gt;

&lt;p&gt;In the example bellow&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, the algorithm will choose to play upper right, because it&#39;s the best move (otherwise, the algorithm loses).
&lt;figure&gt;&lt;img src=&#34;./minmax.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;h4 id=&#34;minmax-with-levels&#34;&gt;Minmax with levels&lt;/h4&gt;

&lt;p&gt;Since our goal is to create an &amp;quot;AI&amp;quot; with several levels of difficulty, implementing a full version of minmax will create an unbeatable algorithm (since the number of states is small, and we can evaluate any tree down to the leaves), and the best we can hope to do against it is a tie. For this effect, in order to create several levels, we will evaluate the tree not up to the leaves, but only up to a predefined depth, this will allow us to create 3 levels of difficulty:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Easy&lt;/strong&gt;: using a depth of &lt;strong&gt;1&lt;/strong&gt;. This will create an AI that plays randomly, but will not waste the chance if it can win the game in it next turn.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Medium&lt;/strong&gt;: using a depth of &lt;strong&gt;2&lt;/strong&gt;. This will also create an AI that plays randomly, and in addition to not spoiling the chance if it can win the game on the next turn, it will also block the opponent&#39;s movement if it detects that he is winning the game on the next turn. The only way to win against it is by playing a &lt;strong&gt;fork&lt;/strong&gt; (create an opportunity where the player has two ways to win (two non-blocked lines of 2)).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hard&lt;/strong&gt;: using a depth of &lt;strong&gt;9&lt;/strong&gt;. This AI is unbeatable, and one can only hope for a draw.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In addition to this, we will add a random agent (all moves are random) as an initiation to the game.&lt;/p&gt;

&lt;h1 id=&#34;tictactoe-with-reinforcement-learning&#34;&gt;Tic-Tac-Toe with Reinforcement Learning&lt;/h1&gt;

&lt;p&gt;While I was at it, I tried to train a reinforcement learning agent to play Tic Tac Toe only by interacting with the environment. For our case (small number of state-action pairs), the tabular Q learning algorithm is an automatic choice. The basic idea of tabular Q-learning is simple: create a table consisting of all possible states on one axis and all possible actions on another axis.The value of each cell in this table is the Q-value. A high Q-value is good and a low Q-value is bad. If the value of the Q function is known, then the optimal policy is simply the greedy policy (more information can be found be &lt;a href=&#34;../beating-atari&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Q learning algorithm is an iterative algorithm, where we initialize the Q function randomly, and we update it after each training episode using the formula:
&lt;figure&gt;&lt;img src=&#34;./formula.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;I trained my algorithm for 6000 episode using a learning rate &lt;code&gt;alpha = 0.3&lt;/code&gt;, and using a epsilon-greedy policy, starting with an &lt;code&gt;epsilon = 0.7&lt;/code&gt;, and decaying it through the learning. Regarding the reward, I used a &lt;code&gt;reward = -1&lt;/code&gt; for a lost match, &lt;code&gt;+1&lt;/code&gt; for a won match, and &lt;code&gt;0.5&lt;/code&gt; for a tie. For state generation, I trained my algorithm against a random agent, because I found that the results are better than playing against a minmax agent. All the training code is available on the github repo mentioned above.&lt;/p&gt;

&lt;h1 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h1&gt;

&lt;p&gt;To conclude, I wanted to see which of the three agents is the best, the random agent (clearly not), the minmax agent level &amp;quot;hard&amp;quot;, or the QL agent. To do this, I made them play against each other for 1000 matches, where each time one of them starts playing first. The results are summarized in the table below.
&lt;figure&gt;&lt;img src=&#34;./score.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;We can see that both algorithms (minmax and Q learning) are unbeatable, but we can notice the superiority of QL over minmax when playing against the random agent. Indeed, the QL agent has won more matches against the latter than minmax. This can be explained by the fact that minmax chooses its move by assuming that the opponent will always choose its best move, which is not the case for the random agent, whereas the QL agent chooses its best move by evaluating the state of the game and making no assumptions about the opponent&#39;s move.&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Game_complexity&#34;&gt;https://en.wikipedia.org/wiki/Game_complexity&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;&lt;a href=&#34;https://nestedsoftware.com/2019/06/15/tic-tac-toe-with-the-minimax-algorithm-5988.123625.html&#34;&gt;https://nestedsoftware.com/2019/06/15/tic-tac-toe-with-the-minimax-algorithm-5988.123625.html&lt;/a&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The genesis of beating Atari games</title>
      <link>https://pvnieo.github.io/post/beating-atari/</link>
      <pubDate>Sun, 25 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://pvnieo.github.io/post/beating-atari/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Advances in the field of reinforcement learning are increasing, the aim
being to teach AI to achieve human performance in everyday tasks. One of
the important area of research in reinforcement learning is to train
agents directly from high-dimensional sensory inputs like vision. In
this sense, video games can be considered a good way to test and improve
reinforcement learning algorithms, mainly because they offer several
different scenarios that require different types of policies to be
solved.&lt;/p&gt;

&lt;p&gt;In this blog, we will focus on beating Atari 2600 games, by training
reinforcement learning algorithms to find the optimal control policy
from solely raw video frames. For that, we will present approaches that
harness the high quality of extracted features from high dimensional
state spaces as an image, made by the recent advances of Convolutional
Neural Networks (CNNs) (see &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;). Such features were hand-crafted
under previous successful approaches, which had limited the scope of
application.&lt;/p&gt;

&lt;p&gt;In particular, we will present the &lt;strong&gt;Deep Q-Network (DQN)&lt;/strong&gt; algorithm,
published by the DeepMind team in 2013 &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, marking a milestone in
machine learning, as well as the improvements made by the same team to
their algorithm in 2015 in &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; and &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h1 id=&#34;background&#34;&gt;Background&lt;/h1&gt;

&lt;p&gt;Reinforcement learning is about discovering an &lt;strong&gt;optimal policy&lt;/strong&gt; (how
to map situations to actions), in order to maximize a numerical reward
signal. The agent is not told which action yields the maximum reward,
but must discover it by interacting with the environment and exploring
all possible scenarios.&lt;/p&gt;

&lt;p&gt;One way of finding the optimal policy is through &lt;strong&gt;Q-learning&lt;/strong&gt;. The
latter is based on an action-value function $Q$ (Q for quality), which
assign for each state &lt;strong&gt;s&lt;/strong&gt;, and a possible action &lt;strong&gt;a&lt;/strong&gt;, following a
policy $\pi$, an estimate of the total reward we would achieve starting
at &lt;strong&gt;s&lt;/strong&gt;, taking the action &lt;strong&gt;a&lt;/strong&gt; then following the same policy (This expression is valid for the Infinite time horizon with terminal state setting, which we are interested in in this study):&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[Q^{\pi}(s,a) = \operatorname{\mathbb{E}}[\sum_{t&#39; = t}^{T} \gamma^{t&#39; - t} r_{t&#39;} |s_t = s, a_t = a, \pi]\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;We define the optimal action-value function &lt;span  class=&#34;math&#34;&gt;\(Q^{*}(s,a)\)&lt;/span&gt; as
&lt;span  class=&#34;math&#34;&gt;\(Q^{*}(s,a) = max_{\pi} Q^{\pi}(s,a)\)&lt;/span&gt;, which correspond to the optimal
policy. If &lt;span  class=&#34;math&#34;&gt;\(Q^{*}(s,a)\)&lt;/span&gt; is known for each state and action, then the
optimal policy is nothing more than the &lt;em&gt;greedy policy&lt;/em&gt;:
&lt;span  class=&#34;math&#34;&gt;\(\pi^{*}(s) = argmax_a Q^{*}(s,a)\)&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;The basic idea behind Q-learning algorithm is that the optimal
action-value function obeys the &lt;em&gt;Bellman equation&lt;/em&gt;:
&lt;span  class=&#34;math&#34;&gt;\(Q^{*}(s,a) = \operatorname{\mathbb{E}}_{s&#39; \sim \mathcal{E}} [r + \gamma max_{a&#39;} Q^{*}(s&#39;,a&#39;) | s, a]\)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This will allow us to estimate the value of the action-value function by
using this identity as an iterative update. This algorithm was
introduced by Ch. Watkins &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:6&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:6&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; and it was proven to converge to
the optimal action-value function if there are finite number of states
and each of the state-action pair is presented repeatedly
&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:7&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:7&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;However, in our case, the problem is too large to learn all action
values in all states separately, and the classical Q-learning algorithm
is not adapted. Instead, a function approximator will be used to
approximate the action-value function
$Q(s, a; \theta) \approx Q^{*}(s,a)$. In this project, we will discuss
the approximation using a neural network, commonly called Deep
Q-Learning.&lt;/p&gt;

&lt;p&gt;The neural network is trained to minimize an objective function that
mimic the behavior of the standard Q-learning update rule:
&lt;span  class=&#34;math&#34;&gt;\(\mathcal{L}(\theta) = \operatorname{\mathbb{E}}\big[\big( r + \gamma max_{a&#39;} Q(s&#39;, a&#39;, \theta) - Q(s, a, \theta) \big)^2\big]\)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This objective function is optimized end-to-end by stochastic gradient
descent, using the following gradient:
&lt;span  class=&#34;math&#34;&gt;\(\frac{\partial \mathcal{L}}{\partial \theta} = \operatorname{\mathbb{E}}\big[\big( r + \gamma max_{a&#39;} Q(s&#39;, a&#39;, \theta) - Q(s, a, \theta) \big) \frac{\partial Q(s, a, \theta)}{\partial \theta}\big]
\label{grad}\)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Using non linear function approximator with the reinforcement learning
setting is know to be unstable or even diverge &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:8&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:8&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;,
and this mainly due to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Training data is corelated&lt;/strong&gt;:   Because the learning agent obtains the learning data by interacting
with the environment, this data is not &lt;strong&gt;i.i.d&lt;/strong&gt;. because successive
samples are correlated&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Moving target values&lt;/strong&gt;:   this due to the fact that small changes to the action-value function
Q may have big impact on the policy, so the policy may oscillate,
and this cause the data to swing from one extreme to another&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Scale of rewards and Q-values is unknown&lt;/strong&gt;:   resulting in large changes in loss from update to update, causing
exploding gradient which poorly affect back propagation&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the following, we will show the techniques used to overcome the
causes of the instability of training Deep Q-learning models.&lt;/p&gt;

&lt;p&gt;It should be noted that Deep Q-learning algorithm is &lt;strong&gt;model-free&lt;/strong&gt;,
meaning that it solves the reinforcement learning task by searching over
policy space to find policies that result in better reward without
learning a model of the environment. It’s also &lt;strong&gt;off-policy&lt;/strong&gt;, meaning
that the learned action-value function, $Q$, directly approximates $Q^*$
, the optimal action-value function, independent of the policy being
followed&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:9&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:9&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h1 id=&#34;atari-2600&#34;&gt;Atari 2600&lt;/h1&gt;

&lt;p&gt;The Atari 2600 is a home video game console that was sold in various
versions by Atari Inc. from 1977 to 1992. It introduced or popularized
many arcade video games that are now considered classics, such as Pong,
Breakout, and Space Invaders. Bellemare et.al &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:10&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:10&#34;&gt;11&lt;/a&gt;&lt;/sup&gt; have
introduced in 2013 the Arcade Learning Environment, which is an emulator
for testing and evaluating reinforcement learning algorithms on Atari
2600 games. Since, it’s used as a rigorous testbed for evaluating and
comparing approaches on a very large set of problems.&lt;/p&gt;

&lt;p&gt;A human playing an Atari game sees $210 \times 160 \times 3$ RGB image,
and the game state is updated every time the agent makes an action. The
user can perform 18 actions with the joystick: doing nothing, pressing
the action button, going in one of 8 directions (up, down, left and
right as well as the 4 diagonals) and going in any of these directions
while pressing the button. Depending on the game, only a subset of these
actions is used. For example, only 4 actions are used to control the
Breakout game: doing nothing, moving left or right, and the action
button for summoning the ball.&lt;/p&gt;

&lt;h2 id=&#34;preprocessing&#34;&gt;Preprocessing&lt;/h2&gt;

&lt;p&gt;A preprocessing step is applied to the frames returned by the
environment. In fact, in order to reduce computation complexity, all
frames are preprocessed by converting them to gray-scale, down-sampling
them to have a shape of $110 \times 84$, and cropping the images to have
a size of $84 \times 84 \times 1$ &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:see-figure&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:see-figure&#34;&gt;0&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;./preprocess.jpeg&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Another important preprocessing step is stacking frames. Since one frame
cannot have all the necessary information - such as direction of motion,
speed or acceleration - for the agent to make decisions, a set of the
last four frames will be stacked.&lt;/p&gt;

&lt;p&gt;So at the end of the preprocessing step, each state that will be
observed by the agent has the format of $84 \times 84 \times 4$.This
preprocessing will be denoted by $\phi$.&lt;/p&gt;

&lt;h1 id=&#34;deep-qnetwork&#34;&gt;Deep Q-Network&lt;/h1&gt;

&lt;p&gt;Deep Q-Network is an algorithm introduced first by Mnih et.al &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:3&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;
in NIPS 2013, and an improved version of it &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;13&lt;/a&gt;&lt;/sup&gt; was published on
Nature 2015. DQN combines Q-learning with a flexible deep neural
network, and was tested on a varied and large set of Atari 2600 games,
reaching super human-level performance on many games.&lt;/p&gt;

&lt;h2 id=&#34;basic-dqn&#34;&gt;Basic DQN&lt;/h2&gt;

&lt;p&gt;The basic DQN is a multi-layered neural network that for a given state
$s$ outputs a vector of action values $Q(s, · ; \theta)$, where $\theta$
are the parameters of the network.&lt;/p&gt;

&lt;p&gt;The success of DQN is essentially due to the use of CNNs to extract
relevant features from raw frames, and the use of the experience
replay&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:11&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:11&#34;&gt;14&lt;/a&gt;&lt;/sup&gt; method to stabilize the learning.&lt;/p&gt;

&lt;p&gt;The problem of Online-learning (which is learning from streaming data)is
that the samples arrive in order they are experienced and, as such, are
highly correlated. The other problem is that the samples are immediately
thrown away after using them, as a result, we are not learning from our
experience effectively. The experience replay is performed by storing
the agent experiences at each time-step,
&lt;span  class=&#34;math&#34;&gt;\(e_t = (s_t, a_t, r_t, a_{t+1})\)&lt;/span&gt;, into a data set
$\mathcal{D} = e_1, e_2, ..., e_N$ called &lt;em&gt;replay memory&lt;/em&gt;, and during
each learning step, a &lt;em&gt;random batch&lt;/em&gt; is sampled from $\mathcal{D}$, and
a gradient descent is performed on it. The experience replay method
breaks the correlation in data, bringing the problem back to i.i.d
setting, and also enable the agent to learn from past policies.&lt;/p&gt;

&lt;p&gt;Following the greedy policy only may not be sufficient to discover the
optimal policy since there no exploration. To overcome this, an
$\epsilon$-greedy policy is used, where the greedy action is selected
with a probability of (1 - $\epsilon$), and a random action is selected
with probability $\epsilon$. During training, this $\epsilon$ is
annealed linearly between 1 and 0.1, decreasing with each time-step.&lt;/p&gt;

&lt;p&gt;The full algorithm is presented in &lt;strong&gt;Algorithm 1&lt;/strong&gt;.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;strong&gt;Algorithm 1&lt;/strong&gt; Deep Q-learning with Experience Replay&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Initialize replay memory D to capacity N&lt;br&gt;
nitialize action-value function Q with random weights&lt;br&gt;
&lt;strong&gt;for&lt;/strong&gt; episode = 1, M &lt;strong&gt;do&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Initialise sequence &lt;span  class=&#34;math&#34;&gt;\(s_1 = \left\{ x_1\right\}\)&lt;/span&gt; and preprocessed
sequenced $\phi_1 = \phi(s1)$&lt;br&gt;
&lt;strong&gt;for&lt;/strong&gt; t = 1, T &lt;strong&gt;do&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;With probability $\epsilon$ select a random action $a_t$&lt;br&gt;
otherwise select $a_t = max_a Q^*(\phi(s_t), a; \theta)$&lt;br&gt;
Execute action &lt;span  class=&#34;math&#34;&gt;\(a_t\)&lt;/span&gt; in emulator and observe reward &lt;span  class=&#34;math&#34;&gt;\(r_t\)&lt;/span&gt; and image
&lt;span  class=&#34;math&#34;&gt;\(x_{t+1}\)&lt;/span&gt;&lt;br&gt;
set &lt;span  class=&#34;math&#34;&gt;\(s_{t+1} = s_t\)&lt;/span&gt; and preprocess &lt;span  class=&#34;math&#34;&gt;\(\phi_{t+1} = \phi(s_{t+1})\)&lt;/span&gt;&lt;br&gt;
Store transition&lt;span  class=&#34;math&#34;&gt;\((\phi_t, a_t, r_t, \phi_{t+1})\)&lt;/span&gt; in &lt;span  class=&#34;math&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;&lt;br&gt;
Sample random minibatch of transitions &lt;span  class=&#34;math&#34;&gt;\((\phi_j, a_j, r_j, \phi_{j+1})\)&lt;/span&gt;
from &lt;span  class=&#34;math&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;&lt;br&gt;
Set &lt;span  class=&#34;math&#34;&gt;\(y_j = \left\{
\begin{array}{ll}
r_j &amp; \text{for terminal } \phi_{j+1} \\
r_j + \gamma max_{a&#39;} Q(\phi_{j+1}, a&#39;; \theta) &amp; \text{for non-terminal } \phi_{j+1}
\end{array}
\right.\)&lt;/span&gt;&lt;br&gt;
Perform a gradient descent step on
$\big( y_j - Q(\phi_j, a_j; \theta)    \big)^2$ according to equation
[grad]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;end for&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;end for&lt;/strong&gt;&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;Another technique used to stabilize the training is &lt;em&gt;clipping reward&lt;/em&gt;,
in which all positive rewards are set +1 and all negative rewards are
set -1. This technique aims to limits the scale of the error derivatives
and prevent from the exploding gradient.&lt;/p&gt;

&lt;p&gt;Finally, the architecture of the network is provided in the following table.
The network is a convolutional neural network, composed of two
convolutional layers, followed by a flatten layer, and the final layers
are two fully-connected layers. The network takes as an input a
preprocessed state that has the size of $84 \times 84 \times 4$, as
explained before. The output of the network of size $n = 18$,
corresponding to the number of possible actions.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Layer&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Input&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Filter size&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Stride&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Num filters&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Activation&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Output&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;conv1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span  class=&#34;math&#34;&gt;\(84 \times 84 \times 4\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;$8 \times 8$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ReLU&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span  class=&#34;math&#34;&gt;\(20 \times 20 \times 16\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;conv2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;$20 \times 20 \times 16$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;$4 \times 4$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ReLU&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span  class=&#34;math&#34;&gt;\(9 \times 9 \times 32\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;fc3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;$9 \times 9 \times 32$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;256&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ReLU&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;256&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;fc4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;256&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Linear&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;18&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;improved-dqn&#34;&gt;Improved DQN&lt;/h2&gt;

&lt;p&gt;In 2015, DeepMind published a new version &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;15&lt;/a&gt;&lt;/sup&gt; of their algorithm
DQN, that seems not only to perform the first version, but also achieved
a level comparable to that of a professional human games tester across a
set of 49 games, using the same algorithm, network architecture and
hyperparameters.&lt;/p&gt;

&lt;p&gt;The main differences between the new and the old architecture is the use
of a bigger CNN to catch more complex features, and the use of a target
network.&lt;/p&gt;

&lt;p&gt;The target network is used to prevent the &lt;strong&gt;moving target values&lt;/strong&gt;
problem(see section [background]). The target network $Q-$ is a neural
network parameterized by $\theta^-$ has the same architecture as the
action-value network $Q$, but instead of the latter, whose parameters
$\theta$ are updated at each iteration, the parameters of the target
network are updated periodically, using a copy of the parameters of the
Q-network. Practically, the parameters $\theta^-$ of the $Q-$ network
are updated every 10000 iterations.&lt;/p&gt;

&lt;p&gt;The target network is used to generate the targets $y_i$ in the
Q-learning updates. This modification makes the algorithm more stable
compared to standard online Q-learning, where an update that increases
&lt;span  class=&#34;math&#34;&gt;\(Q(s_t,a_t)\)&lt;/span&gt; often also increases $Q(s_{t+1},a&#39;)$ and hence also
increases the target $y_j$, possibly leading to oscillations or
divergence of the policy.&lt;/p&gt;

&lt;p&gt;The full improved DQN algorithm is presented in &lt;strong&gt;Algorithm 2&lt;/strong&gt;.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;strong&gt;Algorithm 2&lt;/strong&gt; Improved Deep Q-learning with Experience Replay&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Initialize replay memory D to capacity N&lt;br&gt;
Initialize action-value function $Q$ with random weights $\theta$&lt;br&gt;
Initialize e target action-value function $Q^-$ with weights
$\theta^- = \theta$&lt;br&gt;
&lt;strong&gt;for&lt;/strong&gt; episode = 1, M &lt;strong&gt;do&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Initialise sequence &lt;span  class=&#34;math&#34;&gt;\(s_1 = \left\{ x_1\right\}\)&lt;/span&gt; and preprocessed
sequenced $\phi_1 = \phi(s1)$&lt;br&gt;
&lt;strong&gt;for&lt;/strong&gt; t = 1, T &lt;strong&gt;do&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;With probability $\epsilon$ select a random action $a_t$&lt;br&gt;
otherwise select $a_t = max_a Q^*(\phi(s_t), a; \theta)$&lt;br&gt;
Execute action $a_t$ in emulator and observe reward &lt;span  class=&#34;math&#34;&gt;\(r_t\)&lt;/span&gt; and image
&lt;span  class=&#34;math&#34;&gt;\(x_{t+1}\)&lt;/span&gt;&lt;br&gt;
set &lt;span  class=&#34;math&#34;&gt;\(s_{t+1} = s_t\)&lt;/span&gt; and preprocess &lt;span  class=&#34;math&#34;&gt;\(\phi_{t+1} = \phi(s_{t+1})\)&lt;/span&gt;&lt;br&gt;
Store transition&lt;span  class=&#34;math&#34;&gt;\((\phi_t, a_t, r_t, \phi_{t+1})\)&lt;/span&gt; in &lt;span  class=&#34;math&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;&lt;br&gt;
Sample random minibatch of transitions &lt;span  class=&#34;math&#34;&gt;\((\phi_j, a_j, r_j, \phi_{j+1})\)&lt;/span&gt;
from &lt;span  class=&#34;math&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;&lt;br&gt;
Set &lt;span  class=&#34;math&#34;&gt;\( y_j = \left\{
\begin{array}{ll}
r_j &amp; \text{for terminal } \phi_{j+1} \\
r_j + \gamma max_{a&#39;} Q^-(\phi_{j+1}, a&#39;; \theta^-) &amp; \text{for non-terminal } \phi_{j+1}
\end{array}
\right.\)&lt;/span&gt;&lt;br&gt;
Perform a gradient descent step on
$\big( y_j - Q(\phi_j, a_j; \theta)    \big)^2$ with respect to the
network parameters $\theta$&lt;br&gt;
Every C steps, reset $Q^- = Q$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;end for&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;end for&lt;/strong&gt;&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Another improvement made to the basic DQN algorithm is the use of
&lt;em&gt;Hubber loss&lt;/em&gt;. Instead of the squared error loss
while computing the error term from
&lt;span  class=&#34;math&#34;&gt;\( r_j + \gamma max_{a&#39;} Q^-(\phi_{j+1}, a&#39;; \theta^-) - Q^-(\phi_{j+1}, a&#39;; \theta^-) \)&lt;/span&gt;,
which had the property of being less sensitive to outliers in data than
the squared error loss. This form of error clipping further improved the
stability of the algorithm &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:4&#34;&gt;16&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\label{hubber}
\mathcal{L}(a) = \left\{
\begin{array}{ll}
\frac{1}{2} a^2  &amp; \text{for } |a| \leq 1\\
|a| - \frac{1}{2} &amp; \text{otherwise}
\end{array}
\right.\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Finally, the improved DQN used a bigger CNN. In particular, the number
of filters is doubled, and a convolutional layer is added before the
first fully connected layer. Details of the architecture are provided in the
table bellow.&lt;/p&gt;

&lt;p&gt;When evaluated, the improved DQN outperforms the best existing
reinforcement learning methods on 43 of the games without incorporating
any of the additional prior knowledge about Atari 2600 games used by
other approaches. Furthermore, it has performed at a level that was
comparable to that of a professional human games tester across the set
of 49 games, achieving more than 75% of the human score on more than
half of the games (29 games).&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Layer&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Input&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Filter size&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Stride&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Num filters&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Activation&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Output&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;conv1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span  class=&#34;math&#34;&gt;\(84 \times 84 \times 4\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;$8 \times 8$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ReLU&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span  class=&#34;math&#34;&gt;\(20 \times 20 \times 32\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;conv2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;$20 \times 20 \times 32$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;$4 \times 4$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;64&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ReLU&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span  class=&#34;math&#34;&gt;\(9 \times 9 \times 64\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;conv3&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;$9 \times 9 \times 64$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;$3 \times 3$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;64&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ReLU&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span  class=&#34;math&#34;&gt;\(7 \times 7 \times 64\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;fc4&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;$7 \times 7 \times 64$&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;256&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;ReLU&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;512&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;fc5&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;512&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Linear&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;18&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;double-dqn&#34;&gt;Double DQN&lt;/h2&gt;

&lt;p&gt;Double DQN (DDQN) is another improvement to the previous DQN algorithm.
In fact, another team from DeepMind showed in their publication
&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;17&lt;/a&gt;&lt;/sup&gt; that the previous deep Q-Learning algorithm has tendency to
overestimate the Q function value, due to the max in the formula used to
set targets: &lt;span  class=&#34;math&#34;&gt;\(Q(s,a) \rightarrow r + \gamma max_a Q(s&#39;, a)\)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This overestimation wouldn’t cause any problems if all the actions were
equally overestimated, but the case is, that once one specific action
becomes overestimated, it’s more likely to be chosen in the next
iteration making it very hard for the agent to explore the environment
uniformly and find the right policy.&lt;/p&gt;

&lt;p&gt;A solution to this problem (of overestimation) was proposed by van
Hasselt in &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:12&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:12&#34;&gt;18&lt;/a&gt;&lt;/sup&gt;, and is called Double learning.
In this new
algorithm, two Q functions ( $Q_1$ and $Q_2$ ) are independently
learned. One function is then used to determine the maximizing action
and second to estimate its value. Either $Q_1$ or $Q_2$ is updated
randomly using the same formula as before:
&lt;span  class=&#34;math&#34;&gt;\(Q_i(s,a) \rightarrow r + \gamma Q_j(s&#39;, argmax_a Q_i(s&#39;, a))\)&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Van Hasselt has proven that by decoupling the maximizing action from its
value in this way, one can indeed eliminate the maximization bias
&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:12&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:12&#34;&gt;19&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;The DDQN is based on this idea, and it uses the target network $Q^-$ as
the second value function, without having to introduce additional
networks. Therefore, the greedy policy is evaluated according to the
online network, but using the target network to estimate its value. The
update is the same as Improved DQN, but with minor change in the target
$y_i$:&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[y_i = r_i + \gamma Q(s_{i+1}, argmax_a Q(s_{i+1}, a; \theta); \theta^-)\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The update to the target network stays unchanged from improved DQN, and
remains a periodic copy of the online network.&lt;/p&gt;

&lt;p&gt;In the DDQN paper &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:5&#34;&gt;&lt;a class=&#34;footnote&#34; href=&#34;#fn:5&#34;&gt;20&lt;/a&gt;&lt;/sup&gt;, the authors reports that although DDQN
does not always improve performance, it substantially benefits the
stability of learning. This improved stability directly translates to
ability to learn much complicated tasks.&lt;/p&gt;

&lt;p&gt;When testing DDQN on 49 Atari games, it achieved about twice the average
score of improved DQN with the same hyperparameters. With tuned
hyperparameters, DDQN achieved almost four time the average score of
improved DQN.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;In this study, we have presented the use of deep learning combined with
q-learning to control agents directly from high dimensional sensory
inputs. In particular, we presented the Deep Q-Network algorithm and
it’s variant that marks a milestone in machine learning, by mastering to
play Atari 2600 games based only on raw video frames.&lt;/p&gt;

&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;I. Sutskever A. Krizhevsky and G. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;S. Chintala Y. LeCun. P. Sermanet, K. Kavukcuoglu. Pedestrian detection with unsupervised multi-stage feature learning. In Proc. International Conference onComputer Vision and Pattern Recognition (CVPR 2013), 2013.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;D. Silver A. Graves I. Antonoglou D. Wierstra M. Riedmiller V. Mnih, K. Kavukcuoglu. Playing atari with deep reinforcement learning. 2013. 2012
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;D. Silver A. A. Rusu J. Veness M. G.Bellemare A. Graves M. Riedmiller A. K. Fidjeland G. Ostrovski S. Petersen C. Beattie A. Sadik I. Antonoglou H. King D. Kumaran D. Wierstra S. Legg V. Mnih, K. Kavukcuoglu and D. Hassabis. Human-level control through deep reinforcement learning. 2015
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;D. Silver H. van Hasselt, A. Guez. Deep reinforcement learning with double q-learning. In arXiv:1509.06461v3 [cs.LG], 2015.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;Ch. Watkins. Learning from delayed rewards. 1989
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:6&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;P. Dayan CH Watkins. Q-learning. In Machine Learning, pages 279–292, 1992
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:7&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;B. Van Roy John N. Tsitsiklis. An analysis of temporal-difference learning with function approximation. In IEEETrans. Automat. Contr. 42, pages 674–690, 1997
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:8&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;D. Silver A. A. Rusu J. Veness M. G.Bellemare A. Graves M. Riedmiller A. K. Fidjeland G. Ostrovski S. Petersen C. Beattie A. Sadik I. Antonoglou H. King D. Kumaran D. Wierstra S. Legg V. Mnih, K. Kavukcuoglu and D. Hassabis. Human-level control through deep reinforcement learning. 2015
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. MIT Press, 1998
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:9&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;Joel Veness Marc G Bellemare, Yavar Naddaf and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. In Journal of Artificial Intelligence Research, 47, pages 253–279, 2013
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:10&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;D. Silver A. Graves I. Antonoglou D. Wierstra M. Riedmiller V. Mnih, K. Kavukcuoglu. Playing atari with deep reinforcement learning. 2013. 2012
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;D. Silver A. A. Rusu J. Veness M. G.Bellemare A. Graves M. Riedmiller A. K. Fidjeland G. Ostrovski S. Petersen C. Beattie A. Sadik I. Antonoglou H. King D. Kumaran D. Wierstra S. Legg V. Mnih, K. Kavukcuoglu and D. Hassabis. Human-level control through deep reinforcement learning. 2015
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, DTIC Document, 1993
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:11&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;D. Silver A. A. Rusu J. Veness M. G.Bellemare A. Graves M. Riedmiller A. K. Fidjeland G. Ostrovski S. Petersen C. Beattie A. Sadik I. Antonoglou H. King D. Kumaran D. Wierstra S. Legg V. Mnih, K. Kavukcuoglu and D. Hassabis. Human-level control through deep reinforcement learning. 2015
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;D. Silver A. A. Rusu J. Veness M. G.Bellemare A. Graves M. Riedmiller A. K. Fidjeland G. Ostrovski S. Petersen C. Beattie A. Sadik I. Antonoglou H. King D. Kumaran D. Wierstra S. Legg V. Mnih, K. Kavukcuoglu and D. Hassabis. Human-level control through deep reinforcement learning. 2015
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;D. Silver H. van Hasselt, A. Guez. Deep reinforcement learning with double q-learning. In arXiv:1509.06461v3 [cs.LG], 2015.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;H. van Hasselt. Double q-learning. In Advances in Neural Information Processing Systems, 23, pages 2613–2621, 2010
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:12&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;H. van Hasselt. Double q-learning. In Advances in Neural Information Processing Systems, 23, pages 2613–2621, 2010
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:12&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;D. Silver H. van Hasselt, A. Guez. Deep reinforcement learning with double q-learning. In arXiv:1509.06461v3 [cs.LG], 2015.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:5&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to use the Learning Rate Finder in Pytorch</title>
      <link>https://pvnieo.github.io/post/lr-finder-demo/</link>
      <pubDate>Sat, 10 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://pvnieo.github.io/post/lr-finder-demo/</guid>
      <description>&lt;h1 id=&#34;learning-rate-finder&#34;&gt;Learning rate finder&lt;/h1&gt;

&lt;p&gt;During the training of neural networks, one of the most important hyperparameters, and which largely influences the convergence of the latter, is the learning rate. Choosing a good learning rate is not a simple task, and it is often done using a grid search. However, this can take a long time, especially if the network is very large.&lt;/p&gt;

&lt;p&gt;In his paper &amp;quot;&lt;a href=&#34;https://arxiv.org/pdf/1506.01186.pdf&#34;&gt;Cyclical Learning Rates for Training Neural Networks&lt;/a&gt;&amp;quot;, Leslie Smith presented a method for finding a good learning rate for the majority of gradient based optimizers.&lt;/p&gt;

&lt;p&gt;This method consists in starting the training of the network with a very small learning rate (10-8 for example), and increasing it exponentially with each mini-batch, until reaching a large value (1 or 10), or until the loss diverges (practically, we stop the loop if the loss reached is 4 or 5 times greater than the minimum loss we obtained).&lt;/p&gt;

&lt;p&gt;At the end of the loop, a learning rate is chosen in the region where the loss was minimal. In practice, we take a value an order of magnitude smaller than the one that gave the minimum loss, a value that is always aggressive (to train quickly) but that remains safe in case of an explosion.&lt;/p&gt;

&lt;p&gt;In the following, we will demonstrate the usefulness of this method by using an implementation of this method that we have done. You can find this implementation &lt;a href=&#34;https://github.com/pvnieo/My-Algorithm-Zoo/blob/master/Learning%20rate%20finder/lr_finder.py&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;demo-time&#34;&gt;Demo time&lt;/h2&gt;

&lt;p&gt;First of all, let&#39;s start by importing the necessary packages.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# stdlib
import time
import warnings
import copy
from collections import defaultdict
# 3p
import numpy as np
from tabulate import tabulate
import matplotlib.pyplot as plt
from tqdm import tqdm
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# project
from lr_finder import LRFinder

warnings.filterwarnings(&#39;ignore&#39;)
%reload_ext autoreload
%autoreload 2
%matplotlib inline&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For this demonstration, we will use the MNIST dataset, and we will implement a very simple neural network using pytorch.&lt;/p&gt;

&lt;p&gt;First of all, we define the data loaders and the transformations we will use later.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;batch_size_train = 64
batch_size_test = 1000
## creating dataloaders
train_loader = torch.utils.data.DataLoader(
  torchvision.datasets.MNIST(&#39;/files/&#39;, train=True, download=True,
                             transform=torchvision.transforms.Compose([
                             torchvision.transforms.ToTensor(),
                             torchvision.transforms.Normalize(
                               (0.1307,), (0.3081,))
                             ])),
  batch_size=batch_size_train, shuffle=True)

val_loader = torch.utils.data.DataLoader(
  torchvision.datasets.MNIST(&#39;/files/&#39;, train=False, download=True,
                             transform=torchvision.transforms.Compose([
                               torchvision.transforms.ToTensor(),
                               torchvision.transforms.Normalize(
                                 (0.1307,), (0.3081,))
                             ])),
  batch_size=batch_size_test, shuffle=True)
dataloaders = {&#34;train&#34;: train_loader, &#34;val&#34;: val_loader}&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;examples = enumerate(val_loader)
batch_idx, (example_data, example_targets) = next(examples)
fig = plt.figure()
for i in range(6):
  plt.subplot(2,3,i+1)
  plt.tight_layout()
  plt.imshow(example_data[i][0], cmap=&#39;gray&#39;, interpolation=&#39;none&#39;)
  plt.title(&#34;Ground Truth: {}&#34;.format(example_targets[i]))
  plt.xticks([])
  plt.yticks([])
fig&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;./demo_5_0.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;./demo_5_1.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Now let&#39;s build the network. We will use a simple network, with three convolution layers, and two fully connected layers.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## build the network
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will use the CrossEntropyLoss as a loss because we are facing a classification task, and we will use the famous SGD optimizer to optimize the network.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model = Net()
optimizer = optim.SGD(model.parameters(), lr=0.001)
device = torch.device(&#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;)
criterion = nn.CrossEntropyLoss().to(device)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&#39;s define a helper method that will train the model&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def train_model(model, criterion, optimizer, num_epochs=25):
    since = time.time()

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):

        # Each epoch has a training and validation phase
        for phase in [&#39;train&#39;, &#39;val&#39;]:
            if phase == &#39;train&#39;:
                model.train()  # Set model to training mode
            else:
                model.eval()   # Set model to evaluate mode

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data.
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == &#39;train&#39;):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    # backward + optimize only if in training phase
                    if phase == &#39;train&#39;:
                        loss.backward()
                        optimizer.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)


            # deep copy the model
            if phase == &#39;val&#39; and epoch_acc &gt; best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())


    time_elapsed = time.time() - since

    # load best model weights
    model.load_state_dict(best_model_wts)
    return best_acc.item()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to evaluate this method, we will train the same network several times, using the same hyperparameters, but only change the learning rate. First we will launch the learning rate finder method to find the optimal learning rate.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;model = Net()
optimizer = optim.SGD(model.parameters(), lr=0.1)
lr_finder = LRFinder(model, optimizer, criterion)
lr_finder.find(train_loader)
plt.figure(figsize=(6,6))
lr_finder.plot()&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt; 89%|████████▉ | 89/100 [00:00&lt;00:00, 91.63it/s]


LR Finder is complete. See the graph using `.plot()` method.
Min numerical gradient: 0.39810717055349676
Min loss: 0.7585775750291828&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;./demo_13_2.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;After observing the graph, and following the instructions mentioned above, we will choose the value 0.1 for the learning rate.
As a result, we will compare this value with other learning rate values which are 1e-4, 1e-3, 1e-2, 1 and 10. For each value, we will train the model 5 times, and we will record the average value of the accuracy obtained. The results will be summarized in a table below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lrs = [1e-4, 1e-3, 1e-2, 0.1, 1, 10]
repeat = 5
accs = defaultdict(list)
for lr in tqdm(lrs):
    for _ in range(repeat):
        model = Net().to(device)
        optimizer = optim.SGD(model.parameters(), lr=lr)
        acc = train_model(model, criterion, optimizer, num_epochs=3)
        accs[lr].append(acc)&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;  0%|          | 0/6 [00:00&lt;?, ?it/s]
 17%|█▋        | 1/6 [02:43&lt;13:38, 163.62s/it]
 33%|███▎      | 2/6 [05:26&lt;10:53, 163.29s/it]
 50%|█████     | 3/6 [08:08&lt;08:08, 162.99s/it]
 67%|██████▋   | 4/6 [10:50&lt;05:25, 162.65s/it]
 83%|████████▎ | 5/6 [13:34&lt;02:43, 163.02s/it]
100%|██████████| 6/6 [16:18&lt;00:00, 163.38s/it]&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;means = [&#34;means&#34;] + [np.mean(accs[lr]) for lr in lrs]
stds = [&#34;stds&#34;] + [np.std(accs[lr]) for lr in lrs]
print(tabulate([means, stds], headers=[&#34;lrs&#34;] + lrs))&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;lrs       0.0001      0.001        0.01         0.1           1          10
-----  ---------  ---------  ----------  ----------  ----------  ----------
means  0.17946    0.74826    0.95522     0.98172     0.1073      0.10414
stds   0.0407632  0.0746226  0.00278381  0.00141619  0.00759342  0.00507685&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Et Voila! as we can see, the learning rate suggested by lr_finder gives the best result.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hello World!</title>
      <link>https://pvnieo.github.io/post/hello-world/</link>
      <pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://pvnieo.github.io/post/hello-world/</guid>
      <description>&lt;p&gt;For some time now, I&#39;ve been wanting to start a blog. I hope to find time and energy to keep posting.&lt;/p&gt;

&lt;p&gt;This blog has two main objectives. The first is to consolidate my knowledge through writing; I find that one of the tests to see if I understood something is to try to explain it to someone else. The second is to share what I have learned with the community, in order to help as many people as possible.&lt;/p&gt;

&lt;p&gt;Feedback is appreciated.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
