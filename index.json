[{"authors":["admin"],"categories":null,"content":"Hi everyone! My name is Souhaib, and I was born on 803707632 (Unix standard time).\nI\u0026rsquo;m currently a master student in Mathematics and machine Learning at École Normale Supérieure, Paris.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://pvnieo.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi everyone! My name is Souhaib, and I was born on 803707632 (Unix standard time).\nI\u0026rsquo;m currently a master student in Mathematics and machine Learning at École Normale Supérieure, Paris.","tags":null,"title":"Souhaib Attaiki","type":"authors"},{"authors":null,"categories":null,"content":"Introduction Advances in the field of reinforcement learning are increasing, the aim being to teach AI to achieve human performance in everyday tasks. One of the important area of research in reinforcement learning is to train agents directly from high-dimensional sensory inputs like vision. In this sense, video games can be considered a good way to test and improve reinforcement learning algorithms, mainly because they offer several different scenarios that require different types of policies to be solved.\nIn this blog, we will focus on beating Atari 2600 games, by training reinforcement learning algorithms to find the optimal control policy from solely raw video frames. For that, we will present approaches that harness the high quality of extracted features from high dimensional state spaces as an image, made by the recent advances of Convolutional Neural Networks (CNNs) 12. Such features were hand-crafted under previous successful approaches, which had limited the scope of application.\nIn particular, we will present the Deep Q-Network (DQN) algorithm, published by the DeepMind team in 2013 3, marking a milestone in machine learning, as well as the improvements made by the same team to their algorithm in 2015 in 4 and 5.\nBackground Reinforcement learning is about discovering an optimal policy (how to map situations to actions), in order to maximize a numerical reward signal. The agent is not told which action yields the maximum reward, but must discover it by interacting with the environment and exploring all possible scenarios.\nOne way of finding the optimal policy is through Q-learning. The latter is based on an action-value function $Q$ (Q for quality), which assign for each state s, and a possible action a, following a policy $\\pi$, an estimate of the total reward we would achieve starting at s, taking the action a then following the same policy (This expression is valid for the Infinite time horizon with terminal state setting, which we are interested in in this study):\n\\[Q^{\\pi}(s,a) = \\operatorname{\\mathbb{E}}[\\sum_{t' = t}^{T} \\gamma^{t' - t} r_{t'} |s_t = s, a_t = a, \\pi]\\]\nWe define the optimal action-value function \\(Q^{*}(s,a)\\) as \\(Q^{*}(s,a) = max_{\\pi} Q^{\\pi}(s,a)\\), which correspond to the optimal policy. If \\(Q^{*}(s,a)\\) is known for each state and action, then the optimal policy is nothing more than the greedy policy: \\(\\pi^{*}(s) = argmax_a Q^{*}(s,a)\\).\nThe basic idea behind Q-learning algorithm is that the optimal action-value function obeys the Bellman equation: \\(Q^{*}(s,a) = \\operatorname{\\mathbb{E}}_{s' \\sim \\mathcal{E}} [r + \\gamma max_{a'} Q^{*}(s',a') | s, a]\\)\nThis will allow us to estimate the value of the action-value function by using this identity as an iterative update. This algorithm was introduced by Ch. Watkins 6 and it was proven to converge to the optimal action-value function if there are finite number of states and each of the state-action pair is presented repeatedly 7.\nHowever, in our case, the problem is too large to learn all action values in all states separately, and the classical Q-learning algorithm is not adapted. Instead, a function approximator will be used to approximate the action-value function $Q(s, a; \\theta) \\approx Q^{*}(s,a)$. In this project, we will discuss the approximation using a neural network, commonly called Deep Q-Learning.\nThe neural network is trained to minimize an objective function that mimic the behavior of the standard Q-learning update rule: \\(\\mathcal{L}(\\theta) = \\operatorname{\\mathbb{E}}\\big[\\big( r + \\gamma max_{a'} Q(s', a', \\theta) - Q(s, a, \\theta) \\big)^2\\big]\\)\nThis objective function is optimized end-to-end by stochastic gradient descent, using the following gradient: \\(\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\operatorname{\\mathbb{E}}\\big[\\big( r + \\gamma max_{a'} Q(s', a', \\theta) - Q(s, a, \\theta) \\big) \\frac{\\partial Q(s, a, \\theta)}{\\partial \\theta}\\big] \\label{grad}\\)\nUsing non linear function approximator with the reinforcement learning setting is know to be unstable or even diverge 89, and this mainly due to:\n Training data is corelated: Because the learning agent obtains the learning data by interacting with the environment, this data is not i.i.d. because successive samples are correlated\n Moving target values: this due to the fact that small changes to the action-value function Q may have big impact on the policy, so the policy may oscillate, and this cause the data to swing from one extreme to another\n Scale of rewards and Q-values is unknown: resulting in large changes in loss from update to update, causing exploding gradient which poorly affect back propagation\n  In the following, we will show the techniques used to overcome the causes of the instability of training Deep Q-learning models.\nIt should be noted that Deep Q-learning algorithm is model-free, meaning that it solves the reinforcement learning task by searching over policy space to find policies that result in better reward without learning a model of the environment. It’s also off-policy, meaning that the learned action-value function, $Q$, directly approximates $Q^*$ , the optimal action-value function, independent of the policy being followed10.\nAtari 2600 The Atari 2600 is a home video game console that was sold in various versions by Atari Inc. from 1977 to 1992. It introduced or popularized many arcade video games that are now considered classics, such as Pong, Breakout, and Space Invaders. Bellemare et.al 11 have introduced in 2013 the Arcade Learning Environment, which is an emulator for testing and evaluating reinforcement learning algorithms on Atari 2600 games. Since, it’s used as a rigorous testbed for evaluating and comparing approaches on a very large set of problems.\nA human playing an Atari game sees $210 \\times 160 \\times 3$ RGB image, and the game state is updated every time the agent makes an action. The user can perform 18 actions with the joystick: doing nothing, pressing the action button, going in one of 8 directions (up, down, left and right as well as the 4 diagonals) and going in any of these directions while pressing the button. Depending on the game, only a subset of these actions is used. For example, only 4 actions are used to control the Breakout game: doing nothing, moving left or right, and the action button for summoning the ball.\nPreprocessing A preprocessing step is applied to the frames returned by the environment. In fact, in order to reduce computation complexity, all frames are preprocessed by converting them to gray-scale, down-sampling them to have a shape of $110 \\times 84$, and cropping the images to have a size of $84 \\times 84 \\times 1$ 0.\n\nAnother important preprocessing step is stacking frames. Since one frame cannot have all the necessary information - such as direction of motion, speed or acceleration - for the agent to make decisions, a set of the last four frames will be stacked.\nSo at the end of the preprocessing step, each state that will be observed by the agent has the format of $84 \\times 84 \\times 4$.This preprocessing will be denoted by $\\phi$.\nDeep Q-Network Deep Q-Network is an algorithm introduced first by Mnih et.al 12 in NIPS 2013, and an improved version of it 13 was published on Nature 2015. DQN combines Q-learning with a flexible deep neural network, and was tested on a varied and large set of Atari 2600 games, reaching super human-level performance on many games.\nBasic DQN The basic DQN is a multi-layered neural network that for a given state $s$ outputs a vector of action values $Q(s, · ; \\theta)$, where $\\theta$ are the parameters of the network.\nThe success of DQN is essentially due to the use of CNNs to extract relevant features from raw frames, and the use of the experience replay14 method to stabilize the learning.\nThe problem of Online-learning (which is learning from streaming data)is that the samples arrive in order they are experienced and, as such, are highly correlated. The other problem is that the samples are immediately thrown away after using them, as a result, we are not learning from our experience effectively. The experience replay is performed by storing the agent experiences at each time-step, \\(e_t = (s_t, a_t, r_t, a_{t+1})\\), into a data set $\\mathcal{D} = e_1, e_2, ..., e_N$ called replay memory, and during each learning step, a random batch is sampled from $\\mathcal{D}$, and a gradient descent is performed on it. The experience replay method breaks the correlation in data, bringing the problem back to i.i.d setting, and also enable the agent to learn from past policies.\nFollowing the greedy policy only may not be sufficient to discover the optimal policy since there no exploration. To overcome this, an $\\epsilon$-greedy policy is used, where the greedy action is selected with a probability of (1 - $\\epsilon$), and a random action is selected with probability $\\epsilon$. During training, this $\\epsilon$ is annealed linearly between 1 and 0.1, decreasing with each time-step.\nThe full algorithm is presented in Algorithm 1.\n Algorithm 1 Deep Q-learning with Experience Replay\n Initialize replay memory D to capacity N\nnitialize action-value function Q with random weights\nfor episode = 1, M do\n Initialise sequence \\(s_1 = \\left\\{ x_1\\right\\}\\) and preprocessed sequenced $\\phi_1 = \\phi(s1)$\nfor t = 1, T do\n With probability $\\epsilon$ select a random action $a_t$\notherwise select $a_t = max_a Q^*(\\phi(s_t), a; \\theta)$\nExecute action \\(a_t\\) in emulator and observe reward \\(r_t\\) and image \\(x_{t+1}\\)\nset \\(s_{t+1} = s_t\\) and preprocess \\(\\phi_{t+1} = \\phi(s_{t+1})\\)\nStore transition\\((\\phi_t, a_t, r_t, \\phi_{t+1})\\) in \\(\\mathcal{D}\\)\nSample random minibatch of transitions \\((\\phi_j, a_j, r_j, \\phi_{j+1})\\) from \\(\\mathcal{D}\\)\nSet \\(y_j = \\left\\{ \\begin{array}{ll} r_j \u0026 \\text{for terminal } \\phi_{j+1} \\\\ r_j + \\gamma max_{a'} Q(\\phi_{j+1}, a'; \\theta) \u0026 \\text{for non-terminal } \\phi_{j+1} \\end{array} \\right.\\)\nPerform a gradient descent step on $\\big( y_j - Q(\\phi_j, a_j; \\theta) \\big)^2$ according to equation [grad]\n end for\n end for\n \nAnother technique used to stabilize the training is clipping reward, in which all positive rewards are set +1 and all negative rewards are set -1. This technique aims to limits the scale of the error derivatives and prevent from the exploding gradient.\nFinally, the architecture of the network is provided in the following table. The network is a convolutional neural network, composed of two convolutional layers, followed by a flatten layer, and the final layers are two fully-connected layers. The network takes as an input a preprocessed state that has the size of $84 \\times 84 \\times 4$, as explained before. The output of the network of size $n = 18$, corresponding to the number of possible actions.\n   Layer Input Filter size Stride Num filters Activation Output     conv1 \\(84 \\times 84 \\times 4\\) $8 \\times 8$ 4 16 ReLU \\(20 \\times 20 \\times 16\\)   conv2 $20 \\times 20 \\times 16$ $4 \\times 4$ 2 32 ReLU \\(9 \\times 9 \\times 32\\)   fc3 $9 \\times 9 \\times 32$   256 ReLU 256   fc4 256   18 Linear 18    Improved DQN In 2015, DeepMind published a new version 15 of their algorithm DQN, that seems not only to perform the first version, but also achieved a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters.\nThe main differences between the new and the old architecture is the use of a bigger CNN to catch more complex features, and the use of a target network.\nThe target network is used to prevent the moving target values problem(see section [background]). The target network $Q-$ is a neural network parameterized by $\\theta^-$ has the same architecture as the action-value network $Q$, but instead of the latter, whose parameters $\\theta$ are updated at each iteration, the parameters of the target network are updated periodically, using a copy of the parameters of the Q-network. Practically, the parameters $\\theta^-$ of the $Q-$ network are updated every 10000 iterations.\nThe target network is used to generate the targets $y_i$ in the Q-learning updates. This modification makes the algorithm more stable compared to standard online Q-learning, where an update that increases \\(Q(s_t,a_t)\\) often also increases $Q(s_{t+1},a')$ and hence also increases the target $y_j$, possibly leading to oscillations or divergence of the policy.\nThe full improved DQN algorithm is presented in Algorithm 2.\n Algorithm 2 Improved Deep Q-learning with Experience Replay\n Initialize replay memory D to capacity N\nInitialize action-value function $Q$ with random weights $\\theta$\nInitialize e target action-value function $Q^-$ with weights $\\theta^- = \\theta$\nfor episode = 1, M do\n Initialise sequence \\(s_1 = \\left\\{ x_1\\right\\}\\) and preprocessed sequenced $\\phi_1 = \\phi(s1)$\nfor t = 1, T do\n With probability $\\epsilon$ select a random action $a_t$\notherwise select $a_t = max_a Q^*(\\phi(s_t), a; \\theta)$\nExecute action $a_t$ in emulator and observe reward \\(r_t\\) and image \\(x_{t+1}\\)\nset \\(s_{t+1} = s_t\\) and preprocess \\(\\phi_{t+1} = \\phi(s_{t+1})\\)\nStore transition\\((\\phi_t, a_t, r_t, \\phi_{t+1})\\) in \\(\\mathcal{D}\\)\nSample random minibatch of transitions \\((\\phi_j, a_j, r_j, \\phi_{j+1})\\) from \\(\\mathcal{D}\\)\nSet \\( y_j = \\left\\{ \\begin{array}{ll} r_j \u0026 \\text{for terminal } \\phi_{j+1} \\\\ r_j + \\gamma max_{a'} Q^-(\\phi_{j+1}, a'; \\theta^-) \u0026 \\text{for non-terminal } \\phi_{j+1} \\end{array} \\right.\\)\nPerform a gradient descent step on $\\big( y_j - Q(\\phi_j, a_j; \\theta) \\big)^2$ with respect to the network parameters $\\theta$\nEvery C steps, reset $Q^- = Q$\n end for\n end for\n Another improvement made to the basic DQN algorithm is the use of Hubber loss. Instead of the squared error loss while computing the error term from \\( r_j + \\gamma max_{a'} Q^-(\\phi_{j+1}, a'; \\theta^-) - Q^-(\\phi_{j+1}, a'; \\theta^-) \\), which had the property of being less sensitive to outliers in data than the squared error loss. This form of error clipping further improved the stability of the algorithm 16.\n\\[\\label{hubber} \\mathcal{L}(a) = \\left\\{ \\begin{array}{ll} \\frac{1}{2} a^2 \u0026 \\text{for } |a| \\leq 1\\\\ |a| - \\frac{1}{2} \u0026 \\text{otherwise} \\end{array} \\right.\\]\nFinally, the improved DQN used a bigger CNN. In particular, the number of filters is doubled, and a convolutional layer is added before the first fully connected layer. Details of the architecture are provided in the table bellow.\nWhen evaluated, the improved DQN outperforms the best existing reinforcement learning methods on 43 of the games without incorporating any of the additional prior knowledge about Atari 2600 games used by other approaches. Furthermore, it has performed at a level that was comparable to that of a professional human games tester across the set of 49 games, achieving more than 75% of the human score on more than half of the games (29 games).\n   Layer Input Filter size Stride Num filters Activation Output     conv1 \\(84 \\times 84 \\times 4\\) $8 \\times 8$ 4 32 ReLU \\(20 \\times 20 \\times 32\\)   conv2 $20 \\times 20 \\times 32$ $4 \\times 4$ 2 64 ReLU \\(9 \\times 9 \\times 64\\)   conv3 $9 \\times 9 \\times 64$ $3 \\times 3$ 1 64 ReLU \\(7 \\times 7 \\times 64\\)   fc4 $7 \\times 7 \\times 64$   256 ReLU 512   fc5 512   18 Linear 18    Double DQN Double DQN (DDQN) is another improvement to the previous DQN algorithm. In fact, another team from DeepMind showed in their publication 17 that the previous deep Q-Learning algorithm has tendency to overestimate the Q function value, due to the max in the formula used to set targets: \\(Q(s,a) \\rightarrow r + \\gamma max_a Q(s', a)\\)\nThis overestimation wouldn’t cause any problems if all the actions were equally overestimated, but the case is, that once one specific action becomes overestimated, it’s more likely to be chosen in the next iteration making it very hard for the agent to explore the environment uniformly and find the right policy.\nA solution to this problem (of overestimation) was proposed by van Hasselt in 18, and is called Double learning. In this new algorithm, two Q functions ( $Q_1$ and $Q_2$ ) are independently learned. One function is then used to determine the maximizing action and second to estimate its value. Either $Q_1$ or $Q_2$ is updated randomly using the same formula as before: \\(Q_i(s,a) \\rightarrow r + \\gamma Q_j(s', argmax_a Q_i(s', a))\\)\nVan Hasselt has proven that by decoupling the maximizing action from its value in this way, one can indeed eliminate the maximization bias 19.\nThe DDQN is based on this idea, and it uses the target network $Q^-$ as the second value function, without having to introduce additional networks. Therefore, the greedy policy is evaluated according to the online network, but using the target network to estimate its value. The update is the same as Improved DQN, but with minor change in the target $y_i$:\n\\[y_i = r_i + \\gamma Q(s_{i+1}, argmax_a Q(s_{i+1}, a; \\theta); \\theta^-)\\]\nThe update to the target network stays unchanged from improved DQN, and remains a periodic copy of the online network.\nIn the DDQN paper 20, the authors reports that although DDQN does not always improve performance, it substantially benefits the stability of learning. This improved stability directly translates to ability to learn much complicated tasks.\nWhen testing DDQN on 49 Atari games, it achieved about twice the average score of improved DQN with the same hyperparameters. With tuned hyperparameters, DDQN achieved almost four time the average score of improved DQN.\nConclusion In this study, we have presented the use of deep learning combined with q-learning to control agents directly from high dimensional sensory inputs. In particular, we presented the Deep Q-Network algorithm and it’s variant that marks a milestone in machine learning, by mastering to play Atari 2600 games based only on raw video frames.\nReferences   I. Sutskever A. Krizhevsky and G. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114, 2012. ^ S. Chintala Y. LeCun. P. Sermanet, K. Kavukcuoglu. Pedestrian detection with unsupervised multi-stage feature learning. In Proc. International Conference onComputer Vision and Pattern Recognition (CVPR 2013), 2013. ^ D. Silver A. Graves I. Antonoglou D. Wierstra M. Riedmiller V. Mnih, K. Kavukcuoglu. Playing atari with deep reinforcement learning. 2013. 2012 ^ D. Silver A. A. Rusu J. Veness M. G.Bellemare A. Graves M. Riedmiller A. K. Fidjeland G. Ostrovski S. Petersen C. Beattie A. Sadik I. Antonoglou H. King D. Kumaran D. Wierstra S. Legg V. Mnih, K. Kavukcuoglu and D. Hassabis. Human-level control through deep reinforcement learning. 2015 ^ D. Silver H. van Hasselt, A. Guez. Deep reinforcement learning with double q-learning. In arXiv:1509.06461v3 [cs.LG], 2015. ^ Ch. Watkins. Learning from delayed rewards. 1989 ^ P. Dayan CH Watkins. Q-learning. In Machine Learning, pages 279–292, 1992 ^ B. Van Roy John N. Tsitsiklis. An analysis of temporal-difference learning with function approximation. In IEEETrans. Automat. Contr. 42, pages 674–690, 1997 ^ D. Silver A. A. Rusu J. Veness M. G.Bellemare A. Graves M. Riedmiller A. K. Fidjeland G. Ostrovski S. Petersen C. Beattie A. Sadik I. Antonoglou H. King D. Kumaran D. Wierstra S. Legg V. Mnih, K. Kavukcuoglu and D. Hassabis. Human-level control through deep reinforcement learning. 2015 ^ Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. MIT Press, 1998 ^ Joel Veness Marc G Bellemare, Yavar Naddaf and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. In Journal of Artificial Intelligence Research, 47, pages 253–279, 2013 ^ D. Silver A. Graves I. Antonoglou D. Wierstra M. Riedmiller V. Mnih, K. Kavukcuoglu. Playing atari with deep reinforcement learning. 2013. 2012 ^ D. Silver A. A. Rusu J. Veness M. G.Bellemare A. Graves M. Riedmiller A. K. Fidjeland G. Ostrovski S. Petersen C. Beattie A. Sadik I. Antonoglou H. King D. Kumaran D. Wierstra S. Legg V. Mnih, K. Kavukcuoglu and D. Hassabis. Human-level control through deep reinforcement learning. 2015 ^ Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, DTIC Document, 1993 ^ D. Silver A. A. Rusu J. Veness M. G.Bellemare A. Graves M. Riedmiller A. K. Fidjeland G. Ostrovski S. Petersen C. Beattie A. Sadik I. Antonoglou H. King D. Kumaran D. Wierstra S. Legg V. Mnih, K. Kavukcuoglu and D. Hassabis. Human-level control through deep reinforcement learning. 2015 ^ D. Silver A. A. Rusu J. Veness M. G.Bellemare A. Graves M. Riedmiller A. K. Fidjeland G. Ostrovski S. Petersen C. Beattie A. Sadik I. Antonoglou H. King D. Kumaran D. Wierstra S. Legg V. Mnih, K. Kavukcuoglu and D. Hassabis. Human-level control through deep reinforcement learning. 2015 ^ D. Silver H. van Hasselt, A. Guez. Deep reinforcement learning with double q-learning. In arXiv:1509.06461v3 [cs.LG], 2015. ^ H. van Hasselt. Double q-learning. In Advances in Neural Information Processing Systems, 23, pages 2613–2621, 2010 ^ H. van Hasselt. Double q-learning. In Advances in Neural Information Processing Systems, 23, pages 2613–2621, 2010 ^ D. Silver H. van Hasselt, A. Guez. Deep reinforcement learning with double q-learning. In arXiv:1509.06461v3 [cs.LG], 2015. ^   ","date":1566691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566691200,"objectID":"fbdd884f30ab59c03c8418ad5006738e","permalink":"https://pvnieo.github.io/post/beating-atari/","publishdate":"2019-08-25T00:00:00Z","relpermalink":"/post/beating-atari/","section":"post","summary":"Introduction Advances in the field of reinforcement learning are increasing, the aim being to teach AI to achieve human performance in everyday tasks. One of the important area of research in reinforcement learning is to train agents directly from high-dimensional sensory inputs like vision. In this sense, video games can be considered a good way to test and improve reinforcement learning algorithms, mainly because they offer several different scenarios that require different types of policies to be solved.","tags":["Reinforcement Learning","DQN","Atari 2600","Python"],"title":"The genesis of beating Atari games","type":"post"},{"authors":null,"categories":null,"content":"Learning rate finder During the training of neural networks, one of the most important hyperparameters, and which largely influences the convergence of the latter, is the learning rate. Choosing a good learning rate is not a simple task, and it is often done using a grid search. However, this can take a long time, especially if the network is very large.\nIn his paper \u0026quot;Cyclical Learning Rates for Training Neural Networks\u0026quot;, Leslie Smith presented a method for finding a good learning rate for the majority of gradient based optimizers.\nThis method consists in starting the training of the network with a very small learning rate (10-8 for example), and increasing it exponentially with each mini-batch, until reaching a large value (1 or 10), or until the loss diverges (practically, we stop the loop if the loss reached is 4 or 5 times greater than the minimum loss we obtained).\nAt the end of the loop, a learning rate is chosen in the region where the loss was minimal. In practice, we take a value an order of magnitude smaller than the one that gave the minimum loss, a value that is always aggressive (to train quickly) but that remains safe in case of an explosion.\nIn the following, we will demonstrate the usefulness of this method by using an implementation of this method that we have done. You can find this implementation here.\nDemo time First of all, let's start by importing the necessary packages.\n# stdlib import time import warnings import copy from collections import defaultdict # 3p import numpy as np from tabulate import tabulate import matplotlib.pyplot as plt from tqdm import tqdm import torch import torchvision import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # project from lr_finder import LRFinder warnings.filterwarnings('ignore') %reload_ext autoreload %autoreload 2 %matplotlib inline  For this demonstration, we will use the MNIST dataset, and we will implement a very simple neural network using pytorch.\nFirst of all, we define the data loaders and the transformations we will use later.\nbatch_size_train = 64 batch_size_test = 1000 ## creating dataloaders train_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST('/files/', train=True, download=True, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize( (0.1307,), (0.3081,)) ])), batch_size=batch_size_train, shuffle=True) val_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST('/files/', train=False, download=True, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize( (0.1307,), (0.3081,)) ])), batch_size=batch_size_test, shuffle=True) dataloaders = {\u0026quot;train\u0026quot;: train_loader, \u0026quot;val\u0026quot;: val_loader}  examples = enumerate(val_loader) batch_idx, (example_data, example_targets) = next(examples) fig = plt.figure() for i in range(6): plt.subplot(2,3,i+1) plt.tight_layout() plt.imshow(example_data[i][0], cmap='gray', interpolation='none') plt.title(\u0026quot;Ground Truth: {}\u0026quot;.format(example_targets[i])) plt.xticks([]) plt.yticks([]) fig  \n\nNow let's build the network. We will use a simple network, with three convolution layers, and two fully connected layers.\n## build the network class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x)  We will use the CrossEntropyLoss as a loss because we are facing a classification task, and we will use the famous SGD optimizer to optimize the network.\nmodel = Net() optimizer = optim.SGD(model.parameters(), lr=0.001) device = torch.device(\u0026quot;cuda:0\u0026quot; if torch.cuda.is_available() else \u0026quot;cpu\u0026quot;) criterion = nn.CrossEntropyLoss().to(device)  Let's define a helper method that will train the model\ndef train_model(model, criterion, optimizer, num_epochs=25): since = time.time() best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): # Each epoch has a training and validation phase for phase in ['train', 'val']: if phase == 'train': model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == 'train'): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == 'train': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) epoch_loss = running_loss / len(dataloaders[phase].dataset) epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset) # deep copy the model if phase == 'val' and epoch_acc \u0026gt; best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) time_elapsed = time.time() - since # load best model weights model.load_state_dict(best_model_wts) return best_acc.item()  In order to evaluate this method, we will train the same network several times, using the same hyperparameters, but only change the learning rate. First we will launch the learning rate finder method to find the optimal learning rate.\nmodel = Net() optimizer = optim.SGD(model.parameters(), lr=0.1) lr_finder = LRFinder(model, optimizer, criterion) lr_finder.find(train_loader) plt.figure(figsize=(6,6)) lr_finder.plot()   89%|████████▉ | 89/100 [00:00\u0026lt;00:00, 91.63it/s] LR Finder is complete. See the graph using `.plot()` method. Min numerical gradient: 0.39810717055349676 Min loss: 0.7585775750291828  \nAfter observing the graph, and following the instructions mentioned above, we will choose the value 0.1 for the learning rate. As a result, we will compare this value with other learning rate values which are 1e-4, 1e-3, 1e-2, 1 and 10. For each value, we will train the model 5 times, and we will record the average value of the accuracy obtained. The results will be summarized in a table below.\nlrs = [1e-4, 1e-3, 1e-2, 0.1, 1, 10] repeat = 5 accs = defaultdict(list) for lr in tqdm(lrs): for _ in range(repeat): model = Net().to(device) optimizer = optim.SGD(model.parameters(), lr=lr) acc = train_model(model, criterion, optimizer, num_epochs=3) accs[lr].append(acc)   0%| | 0/6 [00:00\u0026lt;?, ?it/s] 17%|█▋ | 1/6 [02:43\u0026lt;13:38, 163.62s/it] 33%|███▎ | 2/6 [05:26\u0026lt;10:53, 163.29s/it] 50%|█████ | 3/6 [08:08\u0026lt;08:08, 162.99s/it] 67%|██████▋ | 4/6 [10:50\u0026lt;05:25, 162.65s/it] 83%|████████▎ | 5/6 [13:34\u0026lt;02:43, 163.02s/it] 100%|██████████| 6/6 [16:18\u0026lt;00:00, 163.38s/it]  means = [\u0026quot;means\u0026quot;] + [np.mean(accs[lr]) for lr in lrs] stds = [\u0026quot;stds\u0026quot;] + [np.std(accs[lr]) for lr in lrs] print(tabulate([means, stds], headers=[\u0026quot;lrs\u0026quot;] + lrs))  lrs 0.0001 0.001 0.01 0.1 1 10 ----- --------- --------- ---------- ---------- ---------- ---------- means 0.17946 0.74826 0.95522 0.98172 0.1073 0.10414 stds 0.0407632 0.0746226 0.00278381 0.00141619 0.00759342 0.00507685  Et Voila! as we can see, the learning rate suggested by lr_finder gives the best result.\n","date":1565395200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565395200,"objectID":"8aa9522da6871ccd91022e796e5d4280","permalink":"https://pvnieo.github.io/post/lr-finder-demo/","publishdate":"2019-08-10T00:00:00Z","relpermalink":"/post/lr-finder-demo/","section":"post","summary":"Learning rate finder During the training of neural networks, one of the most important hyperparameters, and which largely influences the convergence of the latter, is the learning rate. Choosing a good learning rate is not a simple task, and it is often done using a grid search. However, this can take a long time, especially if the network is very large.\nIn his paper \u0026quot;Cyclical Learning Rates for Training Neural Networks\u0026quot;, Leslie Smith presented a method for finding a good learning rate for the majority of gradient based optimizers.","tags":["Optimization","Learning rate","Pytorch"],"title":"How to use the Learning Rate Finder in Pytorch","type":"post"},{"authors":null,"categories":null,"content":"For some time now, I've been wanting to start a blog. I hope to find time and energy to keep posting.\nThis blog has two main objectives. The first is to consolidate my knowledge through writing; I find that one of the tests to see if I understood something is to try to explain it to someone else. The second is to share what I have learned with the community, in order to help as many people as possible.\nFeedback is appreciated.\n","date":1564185600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564185600,"objectID":"de17d28cdcb6cdb6a4b5e34c8854faa4","permalink":"https://pvnieo.github.io/post/hello-world/","publishdate":"2019-07-27T00:00:00Z","relpermalink":"/post/hello-world/","section":"post","summary":"For some time now, I've been wanting to start a blog. I hope to find time and energy to keep posting.\nThis blog has two main objectives. The first is to consolidate my knowledge through writing; I find that one of the tests to see if I understood something is to try to explain it to someone else. The second is to share what I have learned with the community, in order to help as many people as possible.","tags":null,"title":"Hello World!","type":"post"},{"authors":null,"categories":null,"content":"This project aims to color black and white mangap ages, based on the corresponding anime adaptation, using Generative Adversarial Nets.\nFor the moment, two types of GANs have been tested: pix2pix and cycle-gan.\nFor more information about our method, see the Project Report.\nSome of the results obtained are displayed in the following image: This project is under development, currently, there are diffuclties to color images containing text.\n","date":1551484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551484800,"objectID":"e417867e9e1d7fdf34d0bdb787b40b0f","permalink":"https://pvnieo.github.io/project/mangan/","publishdate":"2019-03-02T00:00:00Z","relpermalink":"/project/mangan/","section":"project","summary":"Automatic Manga colorization using Generative Adversarial Networks.","tags":["Deep Learning","GAN","Pytorch","Automatic colorization","Python"],"title":"ManGan","type":"project"},{"authors":null,"categories":null,"content":" Implementation of some automatic colorization models using deep neural network:\n Implementation of the regression-based model provided in: \u0026ldquo;Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification\u0026rdquo; Link to the original paper Implementation of a classification-based model inspired in part by Zhang et al. in \u0026ldquo;Colorful Image Colorization\u0026rdquo; [Link to the original paper] and R.Dah in here Implementation of a regression-based model inspired from this Medium blog article  Project report You can consult the project report here (in French)\n","date":1519948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519948800,"objectID":"645aefb8040011e8bc1deeb30352bb95","permalink":"https://pvnieo.github.io/project/deeplevi/","publishdate":"2018-03-02T00:00:00Z","relpermalink":"/project/deeplevi/","section":"project","summary":"Automatic colorization using deep neural networks.","tags":["Deep Learning","CNN","Keras","Automatic colorization","Python"],"title":"Deep Levi","type":"project"},{"authors":null,"categories":null,"content":"The objective of this project is to implement indexing techniques and search models in order to create a small ad-hoc search engine on two static databases of textual documents: CACM and CS276 (Stanford) collections.\nIn particular, a preprocessing step is performed to prepare the text, an uncompressed index and a compressed index of the two corpora have been built, and a Boolean search model and a vector search model have been implemented. The evaluation of both search engines shows a good performance.\nFor more information, see the project report.\n","date":1512172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512172800,"objectID":"99519313a8e5bd9c4e9a83406f565ff8","permalink":"https://pvnieo.github.io/project/searchy/","publishdate":"2017-12-02T00:00:00Z","relpermalink":"/project/searchy/","section":"project","summary":"Implementation of a search engine on the cacm and CS276 (Stanford) collections.","tags":["NLP","Search engine","Python"],"title":"Searchy","type":"project"},{"authors":null,"categories":null,"content":"Set of algorithms that I implemented for some random projects, either because I didn\u0026rsquo;t find an implementation in the language I want, or because the implementation didn\u0026rsquo;t suit me.\nImplemented algorithms so far:\n RPCA: A Python implementation of the Principal Component Pursuit algorithm for robust PCA, based on this Robust Principal Component Analysis paper and this Augmented Lagrange Multiplier Method paper. [demo] A pytorch implementation of Leslie N. Smith\u0026rsquo;s Learning rate finder method, first published in the paper Cyclical Learning Rates for Training Neural Networks. [demo]  ","date":1509580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509580800,"objectID":"ebadedafa33d03d3808cb3cf716904ef","permalink":"https://pvnieo.github.io/project/algozoo/","publishdate":"2017-11-02T00:00:00Z","relpermalink":"/project/algozoo/","section":"project","summary":"Set of algorithms  implemented for some random projects.","tags":["Algorithms","RPCA","Python"],"title":"My Algorithm Zoo","type":"project"},{"authors":null,"categories":null,"content":" What is Project Euler? Project Euler is a set of challenging computational/algorithmic problems. Every so often, new problems are added and old problems are changed to reduce cheating from other sources. You can find the problems here: https://projecteuler.net/problems.\nEvery solved problem has a program written in Python and C++.\n","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"5202b6522c91d25f5387373472141cda","permalink":"https://pvnieo.github.io/project/project-euler/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/project/project-euler/","section":"project","summary":"Collection of solutions to Project Euler in Python3 and C++.","tags":["Algorithms","Optimization","Python","C++"],"title":"Project Euler solutions","type":"project"}]