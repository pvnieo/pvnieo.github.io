[{"authors":["admin"],"categories":null,"content":"Hi everyone! My name is Souhaib, and I was born on 803707632 (Unix standard time).\nI\u0026rsquo;m currently a master student in Mathematics and machine Learning at École Normale Supérieure, Paris.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://pvnieo.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi everyone! My name is Souhaib, and I was born on 803707632 (Unix standard time).\nI\u0026rsquo;m currently a master student in Mathematics and machine Learning at École Normale Supérieure, Paris.","tags":null,"title":"Souhaib Attaiki","type":"authors"},{"authors":null,"categories":null,"content":"Learning rate finder During the training of neural networks, one of the most important hyperparameters, and which largely influences the convergence of the latter, is the learning rate. Choosing a good learning rate is not a simple task, and it is often done using a grid search. However, this can take a long time, especially if the network is very large.\nIn his paper \u0026quot;Cyclical Learning Rates for Training Neural Networks\u0026quot;, Leslie Smith presented a method for finding a good learning rate for the majority of gradient based optimizers.\nThis method consists in starting the training of the network with a very small learning rate (10-8 for example), and increasing it exponentially with each mini-batch, until reaching a large value (1 or 10), or until the loss diverges (practically, we stop the loop if the loss reached is 4 or 5 times greater than the minimum loss we obtained).\nAt the end of the loop, a learning rate is chosen in the region where the loss was minimal. In practice, we take a value an order of magnitude smaller than the one that gave the minimum loss, a value that is always aggressive (to train quickly) but that remains safe in case of an explosion.\nIn the following, we will demonstrate the usefulness of this method by using an implementation of this method that we have done. You can find this implementation here.\nDemo time First of all, let's start by importing the necessary packages.\n# stdlib import time import warnings import copy from collections import defaultdict # 3p import numpy as np from tabulate import tabulate import matplotlib.pyplot as plt from tqdm import tqdm import torch import torchvision import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # project from lr_finder import LRFinder warnings.filterwarnings('ignore') %reload_ext autoreload %autoreload 2 %matplotlib inline  For this demonstration, we will use the MNIST dataset, and we will implement a very simple neural network using pytorch.\nFirst of all, we define the data loaders and the transformations we will use later.\nbatch_size_train = 64 batch_size_test = 1000 ## creating dataloaders train_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST('/files/', train=True, download=True, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize( (0.1307,), (0.3081,)) ])), batch_size=batch_size_train, shuffle=True) val_loader = torch.utils.data.DataLoader( torchvision.datasets.MNIST('/files/', train=False, download=True, transform=torchvision.transforms.Compose([ torchvision.transforms.ToTensor(), torchvision.transforms.Normalize( (0.1307,), (0.3081,)) ])), batch_size=batch_size_test, shuffle=True) dataloaders = {\u0026quot;train\u0026quot;: train_loader, \u0026quot;val\u0026quot;: val_loader}  examples = enumerate(val_loader) batch_idx, (example_data, example_targets) = next(examples) fig = plt.figure() for i in range(6): plt.subplot(2,3,i+1) plt.tight_layout() plt.imshow(example_data[i][0], cmap='gray', interpolation='none') plt.title(\u0026quot;Ground Truth: {}\u0026quot;.format(example_targets[i])) plt.xticks([]) plt.yticks([]) fig  \n\nNow let's build the network. We will use a simple network, with three convolution layers, and two fully connected layers.\n## build the network class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.conv2 = nn.Conv2d(10, 20, kernel_size=5) self.conv2_drop = nn.Dropout2d() self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2)) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = F.dropout(x, training=self.training) x = self.fc2(x) return F.log_softmax(x)  We will use the CrossEntropyLoss as a loss because we are facing a classification task, and we will use the famous SGD optimizer to optimize the network.\nmodel = Net() optimizer = optim.SGD(model.parameters(), lr=0.001) device = torch.device(\u0026quot;cuda:0\u0026quot; if torch.cuda.is_available() else \u0026quot;cpu\u0026quot;) criterion = nn.CrossEntropyLoss().to(device)  Let's define a helper method that will train the model\ndef train_model(model, criterion, optimizer, num_epochs=25): since = time.time() best_model_wts = copy.deepcopy(model.state_dict()) best_acc = 0.0 for epoch in range(num_epochs): # Each epoch has a training and validation phase for phase in ['train', 'val']: if phase == 'train': model.train() # Set model to training mode else: model.eval() # Set model to evaluate mode running_loss = 0.0 running_corrects = 0 # Iterate over data. for inputs, labels in dataloaders[phase]: inputs = inputs.to(device) labels = labels.to(device) # zero the parameter gradients optimizer.zero_grad() # forward # track history if only in train with torch.set_grad_enabled(phase == 'train'): outputs = model(inputs) _, preds = torch.max(outputs, 1) loss = criterion(outputs, labels) # backward + optimize only if in training phase if phase == 'train': loss.backward() optimizer.step() # statistics running_loss += loss.item() * inputs.size(0) running_corrects += torch.sum(preds == labels.data) epoch_loss = running_loss / len(dataloaders[phase].dataset) epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset) # deep copy the model if phase == 'val' and epoch_acc \u0026gt; best_acc: best_acc = epoch_acc best_model_wts = copy.deepcopy(model.state_dict()) time_elapsed = time.time() - since # load best model weights model.load_state_dict(best_model_wts) return best_acc.item()  In order to evaluate this method, we will train the same network several times, using the same hyperparameters, but only change the learning rate. First we will launch the learning rate finder method to find the optimal learning rate.\nmodel = Net() optimizer = optim.SGD(model.parameters(), lr=0.1) lr_finder = LRFinder(model, optimizer, criterion) lr_finder.find(train_loader) plt.figure(figsize=(6,6)) lr_finder.plot()   89%|████████▉ | 89/100 [00:00\u0026lt;00:00, 91.63it/s] LR Finder is complete. See the graph using `.plot()` method. Min numerical gradient: 0.39810717055349676 Min loss: 0.7585775750291828  \nAfter observing the graph, and following the instructions mentioned above, we will choose the value 0.1 for the learning rate. As a result, we will compare this value with other learning rate values which are 1e-4, 1e-3, 1e-2, 1 and 10. For each value, we will train the model 5 times, and we will record the average value of the accuracy obtained. The results will be summarized in a table below.\nlrs = [1e-4, 1e-3, 1e-2, 0.1, 1, 10] repeat = 5 accs = defaultdict(list) for lr in tqdm(lrs): for _ in range(repeat): model = Net().to(device) optimizer = optim.SGD(model.parameters(), lr=lr) acc = train_model(model, criterion, optimizer, num_epochs=3) accs[lr].append(acc)   0%| | 0/6 [00:00\u0026lt;?, ?it/s] 17%|█▋ | 1/6 [02:43\u0026lt;13:38, 163.62s/it] 33%|███▎ | 2/6 [05:26\u0026lt;10:53, 163.29s/it] 50%|█████ | 3/6 [08:08\u0026lt;08:08, 162.99s/it] 67%|██████▋ | 4/6 [10:50\u0026lt;05:25, 162.65s/it] 83%|████████▎ | 5/6 [13:34\u0026lt;02:43, 163.02s/it] 100%|██████████| 6/6 [16:18\u0026lt;00:00, 163.38s/it]  means = [\u0026quot;means\u0026quot;] + [np.mean(accs[lr]) for lr in lrs] stds = [\u0026quot;stds\u0026quot;] + [np.std(accs[lr]) for lr in lrs] print(tabulate([means, stds], headers=[\u0026quot;lrs\u0026quot;] + lrs))  lrs 0.0001 0.001 0.01 0.1 1 10 ----- --------- --------- ---------- ---------- ---------- ---------- means 0.17946 0.74826 0.95522 0.98172 0.1073 0.10414 stds 0.0407632 0.0746226 0.00278381 0.00141619 0.00759342 0.00507685  Et Voila! as we can see, the learning rate suggested by lr_finder gives the best result.\n","date":1565395200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565395200,"objectID":"8aa9522da6871ccd91022e796e5d4280","permalink":"https://pvnieo.github.io/post/lr-finder-demo/","publishdate":"2019-08-10T00:00:00Z","relpermalink":"/post/lr-finder-demo/","section":"post","summary":"Learning rate finder During the training of neural networks, one of the most important hyperparameters, and which largely influences the convergence of the latter, is the learning rate. Choosing a good learning rate is not a simple task, and it is often done using a grid search. However, this can take a long time, especially if the network is very large.\nIn his paper \u0026quot;Cyclical Learning Rates for Training Neural Networks\u0026quot;, Leslie Smith presented a method for finding a good learning rate for the majority of gradient based optimizers.","tags":["Optimization","Learning rate","Pytorch"],"title":"How to use the Learning Rate Finder in Pytorch","type":"post"},{"authors":null,"categories":null,"content":"For some time now, I've been wanting to start a blog. I hope to find time and energy to keep posting.\nThis blog has two main objectives. The first is to consolidate my knowledge through writing; I find that one of the tests to see if I understood something is to try to explain it to someone else. The second is to share what I have learned with the community, in order to help as many people as possible.\nFeedback is appreciated.\n","date":1564185600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564185600,"objectID":"de17d28cdcb6cdb6a4b5e34c8854faa4","permalink":"https://pvnieo.github.io/post/hello-world/","publishdate":"2019-07-27T00:00:00Z","relpermalink":"/post/hello-world/","section":"post","summary":"For some time now, I've been wanting to start a blog. I hope to find time and energy to keep posting.\nThis blog has two main objectives. The first is to consolidate my knowledge through writing; I find that one of the tests to see if I understood something is to try to explain it to someone else. The second is to share what I have learned with the community, in order to help as many people as possible.","tags":null,"title":"Hello World!","type":"post"},{"authors":null,"categories":null,"content":"This project aims to color black and white mangap ages, based on the corresponding anime adaptation, using Generative Adversarial Nets.\nFor the moment, two types of GANs have been tested: pix2pix and cycle-gan.\nFor more information about our method, see the Project Report.\nSome of the results obtained are displayed in the following image: This project is under development, currently, there are diffuclties to color images containing text.\n","date":1551484800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551484800,"objectID":"e417867e9e1d7fdf34d0bdb787b40b0f","permalink":"https://pvnieo.github.io/project/mangan/","publishdate":"2019-03-02T00:00:00Z","relpermalink":"/project/mangan/","section":"project","summary":"Automatic Manga colorization using Generative Adversarial Networks.","tags":["Deep Learning","GAN","Pytorch","Automatic colorization","Python"],"title":"ManGan","type":"project"},{"authors":null,"categories":null,"content":" Implementation of some automatic colorization models using deep neural network:\n Implementation of the regression-based model provided in: \u0026ldquo;Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification\u0026rdquo; Link to the original paper Implementation of a classification-based model inspired in part by Zhang et al. in \u0026ldquo;Colorful Image Colorization\u0026rdquo; [Link to the original paper] and R.Dah in here Implementation of a regression-based model inspired from this Medium blog article  Project report You can consult the project report here (in French)\n","date":1519948800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519948800,"objectID":"645aefb8040011e8bc1deeb30352bb95","permalink":"https://pvnieo.github.io/project/deeplevi/","publishdate":"2018-03-02T00:00:00Z","relpermalink":"/project/deeplevi/","section":"project","summary":"Automatic colorization using deep neural networks.","tags":["Deep Learning","CNN","Keras","Automatic colorization","Python"],"title":"Deep Levi","type":"project"},{"authors":null,"categories":null,"content":"The objective of this project is to implement indexing techniques and search models in order to create a small ad-hoc search engine on two static databases of textual documents: CACM and CS276 (Stanford) collections.\nIn particular, a preprocessing step is performed to prepare the text, an uncompressed index and a compressed index of the two corpora have been built, and a Boolean search model and a vector search model have been implemented. The evaluation of both search engines shows a good performance.\nFor more information, see the project report.\n","date":1512172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512172800,"objectID":"99519313a8e5bd9c4e9a83406f565ff8","permalink":"https://pvnieo.github.io/project/searchy/","publishdate":"2017-12-02T00:00:00Z","relpermalink":"/project/searchy/","section":"project","summary":"Implementation of a search engine on the cacm and CS276 (Stanford) collections.","tags":["NLP","Search engine","Python"],"title":"Searchy","type":"project"},{"authors":null,"categories":null,"content":"Set of algorithms that I implemented for some random projects, either because I didn\u0026rsquo;t find an implementation in the language I want, or because the implementation didn\u0026rsquo;t suit me.\nImplemented algorithms so far:\n RPCA: A Python implementation of the Principal Component Pursuit algorithm for robust PCA, based on this Robust Principal Component Analysis paper and this Augmented Lagrange Multiplier Method paper. [demo] A pytorch implementation of Leslie N. Smith\u0026rsquo;s Learning rate finder method, first published in the paper Cyclical Learning Rates for Training Neural Networks. [demo]  ","date":1509580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509580800,"objectID":"ebadedafa33d03d3808cb3cf716904ef","permalink":"https://pvnieo.github.io/project/algozoo/","publishdate":"2017-11-02T00:00:00Z","relpermalink":"/project/algozoo/","section":"project","summary":"Set of algorithms  implemented for some random projects.","tags":["Algorithms","RPCA","Python"],"title":"My Algorithm Zoo","type":"project"},{"authors":null,"categories":null,"content":" What is Project Euler? Project Euler is a set of challenging computational/algorithmic problems. Every so often, new problems are added and old problems are changed to reduce cheating from other sources. You can find the problems here: https://projecteuler.net/problems.\nEvery solved problem has a program written in Python and C++.\n","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"5202b6522c91d25f5387373472141cda","permalink":"https://pvnieo.github.io/project/project-euler/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/project/project-euler/","section":"project","summary":"Collection of solutions to Project Euler in Python3 and C++.","tags":["Algorithms","Optimization","Python","C++"],"title":"Project Euler solutions","type":"project"}]