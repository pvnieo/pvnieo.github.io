<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pvnieo</title>
    <link>https://pvnieo.github.io/</link>
      <atom:link href="https://pvnieo.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>pvnieo</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019 Souhaib Attaiki, made with lots of ❤️ and steaming hot ☕️.</copyright><lastBuildDate>Sat, 10 Aug 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://pvnieo.github.io/img/icon.png</url>
      <title>pvnieo</title>
      <link>https://pvnieo.github.io/</link>
    </image>
    
    <item>
      <title>How to use the Learning Rate Finder in Pytorch</title>
      <link>https://pvnieo.github.io/post/lr-finder-demo/</link>
      <pubDate>Sat, 10 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://pvnieo.github.io/post/lr-finder-demo/</guid>
      <description>&lt;h1 id=&#34;learning-rate-finder&#34;&gt;Learning rate finder&lt;/h1&gt;

&lt;p&gt;During the training of neural networks, one of the most important hyperparameters, and which largely influences the convergence of the latter, is the learning rate. Choosing a good learning rate is not a simple task, and it is often done using a grid search. However, this can take a long time, especially if the network is very large.&lt;/p&gt;

&lt;p&gt;In his paper &amp;quot;&lt;a href=&#34;https://arxiv.org/pdf/1506.01186.pdf&#34;&gt;Cyclical Learning Rates for Training Neural Networks&lt;/a&gt;&amp;quot;, Leslie Smith presented a method for finding a good learning rate for the majority of gradient based optimizers.&lt;/p&gt;

&lt;p&gt;This method consists in starting the training of the network with a very small learning rate (10-8 for example), and increasing it exponentially with each mini-batch, until reaching a large value (1 or 10), or until the loss diverges (practically, we stop the loop if the loss reached is 4 or 5 times greater than the minimum loss we obtained).&lt;/p&gt;

&lt;p&gt;At the end of the loop, a learning rate is chosen in the region where the loss was minimal. In practice, we take a value an order of magnitude smaller than the one that gave the minimum loss, a value that is always aggressive (to train quickly) but that remains safe in case of an explosion.&lt;/p&gt;

&lt;p&gt;In the following, we will demonstrate the usefulness of this method by using an implementation of this method that we have done. You can find this implementation &lt;a href=&#34;https://github.com/pvnieo/My-Algorithm-Zoo/blob/master/Learning%20rate%20finder/lr_finder.py&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;demo-time&#34;&gt;Demo time&lt;/h2&gt;

&lt;p&gt;First of all, let&#39;s start by importing the necessary packages.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# stdlib
import time
import warnings
import copy
from collections import defaultdict
# 3p
import numpy as np
from tabulate import tabulate
import matplotlib.pyplot as plt
from tqdm import tqdm
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# project
from lr_finder import LRFinder

warnings.filterwarnings(&#39;ignore&#39;)
%reload_ext autoreload
%autoreload 2
%matplotlib inline
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For this demonstration, we will use the MNIST dataset, and we will implement a very simple neural network using pytorch.&lt;/p&gt;

&lt;p&gt;First of all, we define the data loaders and the transformations we will use later.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;batch_size_train = 64
batch_size_test = 1000
## creating dataloaders
train_loader = torch.utils.data.DataLoader(
  torchvision.datasets.MNIST(&#39;/files/&#39;, train=True, download=True,
                             transform=torchvision.transforms.Compose([
                             torchvision.transforms.ToTensor(),
                             torchvision.transforms.Normalize(
                               (0.1307,), (0.3081,))
                             ])),
  batch_size=batch_size_train, shuffle=True)

val_loader = torch.utils.data.DataLoader(
  torchvision.datasets.MNIST(&#39;/files/&#39;, train=False, download=True,
                             transform=torchvision.transforms.Compose([
                               torchvision.transforms.ToTensor(),
                               torchvision.transforms.Normalize(
                                 (0.1307,), (0.3081,))
                             ])),
  batch_size=batch_size_test, shuffle=True)
dataloaders = {&amp;quot;train&amp;quot;: train_loader, &amp;quot;val&amp;quot;: val_loader}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;examples = enumerate(val_loader)
batch_idx, (example_data, example_targets) = next(examples)
fig = plt.figure()
for i in range(6):
  plt.subplot(2,3,i+1)
  plt.tight_layout()
  plt.imshow(example_data[i][0], cmap=&#39;gray&#39;, interpolation=&#39;none&#39;)
  plt.title(&amp;quot;Ground Truth: {}&amp;quot;.format(example_targets[i]))
  plt.xticks([])
  plt.yticks([])
fig
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;./demo_5_0.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;./demo_5_1.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;Now let&#39;s build the network. We will use a simple network, with three convolution layers, and two fully connected layers.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## build the network
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will use the CrossEntropyLoss as a loss because we are facing a classification task, and we will use the famous SGD optimizer to optimize the network.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = Net()
optimizer = optim.SGD(model.parameters(), lr=0.001)
device = torch.device(&amp;quot;cuda:0&amp;quot; if torch.cuda.is_available() else &amp;quot;cpu&amp;quot;)
criterion = nn.CrossEntropyLoss().to(device)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&#39;s define a helper method that will train the model&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def train_model(model, criterion, optimizer, num_epochs=25):
    since = time.time()

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):

        # Each epoch has a training and validation phase
        for phase in [&#39;train&#39;, &#39;val&#39;]:
            if phase == &#39;train&#39;:
                model.train()  # Set model to training mode
            else:
                model.eval()   # Set model to evaluate mode

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data.
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == &#39;train&#39;):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    # backward + optimize only if in training phase
                    if phase == &#39;train&#39;:
                        loss.backward()
                        optimizer.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)


            # deep copy the model
            if phase == &#39;val&#39; and epoch_acc &amp;gt; best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())


    time_elapsed = time.time() - since

    # load best model weights
    model.load_state_dict(best_model_wts)
    return best_acc.item()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to evaluate this method, we will train the same network several times, using the same hyperparameters, but only change the learning rate. First we will launch the learning rate finder method to find the optimal learning rate.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model = Net()
optimizer = optim.SGD(model.parameters(), lr=0.1)
lr_finder = LRFinder(model, optimizer, criterion)
lr_finder.find(train_loader)
plt.figure(figsize=(6,6))
lr_finder.plot()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt; 89%|████████▉ | 89/100 [00:00&amp;lt;00:00, 91.63it/s]


LR Finder is complete. See the graph using `.plot()` method.
Min numerical gradient: 0.39810717055349676
Min loss: 0.7585775750291828
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;./demo_13_2.png&#34; alt=&#34;png&#34;&gt;&lt;/figure&gt;&lt;/p&gt;

&lt;p&gt;After observing the graph, and following the instructions mentioned above, we will choose the value 0.1 for the learning rate.
As a result, we will compare this value with other learning rate values which are 1e-4, 1e-3, 1e-2, 1 and 10. For each value, we will train the model 5 times, and we will record the average value of the accuracy obtained. The results will be summarized in a table below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lrs = [1e-4, 1e-3, 1e-2, 0.1, 1, 10]
repeat = 5
accs = defaultdict(list)
for lr in tqdm(lrs):
    for _ in range(repeat):
        model = Net().to(device)
        optimizer = optim.SGD(model.parameters(), lr=lr)
        acc = train_model(model, criterion, optimizer, num_epochs=3)
        accs[lr].append(acc)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;  0%|          | 0/6 [00:00&amp;lt;?, ?it/s]
 17%|█▋        | 1/6 [02:43&amp;lt;13:38, 163.62s/it]
 33%|███▎      | 2/6 [05:26&amp;lt;10:53, 163.29s/it]
 50%|█████     | 3/6 [08:08&amp;lt;08:08, 162.99s/it]
 67%|██████▋   | 4/6 [10:50&amp;lt;05:25, 162.65s/it]
 83%|████████▎ | 5/6 [13:34&amp;lt;02:43, 163.02s/it]
100%|██████████| 6/6 [16:18&amp;lt;00:00, 163.38s/it]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;means = [&amp;quot;means&amp;quot;] + [np.mean(accs[lr]) for lr in lrs]
stds = [&amp;quot;stds&amp;quot;] + [np.std(accs[lr]) for lr in lrs]
print(tabulate([means, stds], headers=[&amp;quot;lrs&amp;quot;] + lrs))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;lrs       0.0001      0.001        0.01         0.1           1          10
-----  ---------  ---------  ----------  ----------  ----------  ----------
means  0.17946    0.74826    0.95522     0.98172     0.1073      0.10414
stds   0.0407632  0.0746226  0.00278381  0.00141619  0.00759342  0.00507685
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Et Voila! as we can see, the learning rate suggested by lr_finder gives the best result.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hello World!</title>
      <link>https://pvnieo.github.io/post/hello-world/</link>
      <pubDate>Sat, 27 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://pvnieo.github.io/post/hello-world/</guid>
      <description>&lt;p&gt;For some time now, I&#39;ve been wanting to start a blog. I hope to find time and energy to keep posting.&lt;/p&gt;

&lt;p&gt;This blog has two main objectives. The first is to consolidate my knowledge through writing; I find that one of the tests to see if I understood something is to try to explain it to someone else. The second is to share what I have learned with the community, in order to help as many people as possible.&lt;/p&gt;

&lt;p&gt;Feedback is appreciated.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ManGan</title>
      <link>https://pvnieo.github.io/project/mangan/</link>
      <pubDate>Sat, 02 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://pvnieo.github.io/project/mangan/</guid>
      <description>&lt;p&gt;This project aims to color black and white mangap ages, based on the corresponding anime adaptation, using Generative Adversarial Nets.&lt;/p&gt;

&lt;p&gt;For the moment, two types of GANs have been tested: &lt;a href=&#34;https://arxiv.org/pdf/1611.07004.pdf&#34; target=&#34;_blank&#34;&gt;pix2pix&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/pdf/1703.10593.pdf&#34; target=&#34;_blank&#34;&gt;cycle-gan&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For more information about our method, see the &lt;a href=&#34;https://www.researchgate.net/publication/331175026_Manga_Colorization_using_Generative_Adversarial_Nets&#34; target=&#34;_blank&#34;&gt;Project Report&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Some of the results obtained are displayed in the following image:
&lt;img src=&#34;results.jpeg&#34; alt=&#34;GitHub Logo&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This project is &lt;strong&gt;under development&lt;/strong&gt;, currently, there are diffuclties to color images containing text.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Levi</title>
      <link>https://pvnieo.github.io/project/deeplevi/</link>
      <pubDate>Fri, 02 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://pvnieo.github.io/project/deeplevi/</guid>
      <description>

&lt;p&gt;Implementation of some automatic colorization models using deep neural network:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Implementation of the regression-based model provided in: &amp;ldquo;Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification&amp;rdquo; &lt;a href=&#34;http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/en/&#34; target=&#34;_blank&#34;&gt;Link to the original paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Implementation of a classification-based model  inspired in part by Zhang et al. in &amp;ldquo;Colorful Image Colorization&amp;rdquo;  [&lt;a href=&#34;https://github.com/richzhang/colorization&#34; target=&#34;_blank&#34;&gt;Link to the original paper&lt;/a&gt;] and R.Dah in &lt;a href=&#34;http://tinyclouds.org/colorize&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Implementation of a regression-based model inspired from &lt;a href=&#34;https://medium.freecodecamp.org/colorize-b-w-photos-with-a-100-line-neural-network-53d9b4449f8d&#34; target=&#34;_blank&#34;&gt;this Medium blog article&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;project-report&#34;&gt;Project report&lt;/h2&gt;

&lt;p&gt;You can consult the project report &lt;a href=&#34;https://github.com/pvnieo/Deep-Levi/blob/master/%C3%89tude%20comparative%20d%E2%80%99architecture%20de%20colorisation%20automatique.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; (in French)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Searchy</title>
      <link>https://pvnieo.github.io/project/searchy/</link>
      <pubDate>Sat, 02 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://pvnieo.github.io/project/searchy/</guid>
      <description>&lt;p&gt;The objective of this project is to implement indexing techniques and search models in order to create a small ad-hoc search engine on two static databases of textual documents: CACM and CS276 (Stanford) collections.&lt;/p&gt;

&lt;p&gt;In particular, a preprocessing step is performed to prepare the text, an uncompressed index and a compressed index of the two corpora have been built, and a Boolean search model and a vector search model have been implemented. The evaluation of both search engines shows a good performance.&lt;/p&gt;

&lt;p&gt;For more information, see the &lt;a href=&#34;https://github.com/pvnieo/searchy/blob/master/report.ipynb&#34; target=&#34;_blank&#34;&gt;project report&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Algorithm Zoo</title>
      <link>https://pvnieo.github.io/project/algozoo/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://pvnieo.github.io/project/algozoo/</guid>
      <description>&lt;p&gt;Set of algorithms that I implemented for some random projects, either because I didn&amp;rsquo;t find an implementation in the language I want, or because the implementation didn&amp;rsquo;t suit me.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Implemented algorithms&lt;/strong&gt; so far:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RPCA: A Python implementation of the Principal Component Pursuit algorithm for robust PCA, based on this &lt;a href=&#34;https://statweb.stanford.edu/~candes/papers/RobustPCA.pdf&#34; target=&#34;_blank&#34;&gt;Robust Principal Component Analysis paper&lt;/a&gt; and this     &lt;a href=&#34;https://arxiv.org/pdf/1009.5055.pdf&#34; target=&#34;_blank&#34;&gt;Augmented Lagrange Multiplier Method paper&lt;/a&gt;. [&lt;a href=&#34;https://github.com/pvnieo/My-Algorithm-Zoo/blob/master/RPCA/demo.ipynb&#34; target=&#34;_blank&#34;&gt;demo&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;A pytorch implementation of Leslie N. Smith&amp;rsquo;s Learning rate finder method, first published in the paper &lt;a href=&#34;https://arxiv.org/pdf/1506.01186.pdf&#34; target=&#34;_blank&#34;&gt;Cyclical Learning Rates for Training Neural Networks&lt;/a&gt;. [&lt;a href=&#34;https://github.com/pvnieo/My-Algorithm-Zoo/blob/master/Learning%20rate%20finder/demo.ipynb&#34; target=&#34;_blank&#34;&gt;demo&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Project Euler solutions</title>
      <link>https://pvnieo.github.io/project/project-euler/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://pvnieo.github.io/project/project-euler/</guid>
      <description>

&lt;h2 id=&#34;what-is-project-euler&#34;&gt;What is Project Euler?&lt;/h2&gt;

&lt;p&gt;Project Euler is a set of challenging computational/algorithmic problems. Every so often,
new problems are added and old problems are changed to reduce cheating from other sources.
You can find the problems here: &lt;a href=&#34;https://projecteuler.net/problems&#34; target=&#34;_blank&#34;&gt;https://projecteuler.net/problems&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Every solved problem has a program written in Python and C++.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
